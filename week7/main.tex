\documentclass{beamer}

% \usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}
\usepackage{lmodern}   % modern Latin Modern fonts
\usepackage{textcomp}  % provides \textquoteright
\usepackage{lmodern} % Latin Modern fonts with T1 shapes


\usepackage{graphicx}
\usepackage{ragged2e} % for generating dummy text
\usepackage[backend=biber,style=authoryear]{biblatex}
% \addbibresource{references.bib}

\usetheme{Madrid}
\usecolortheme{default}
\usefonttheme{professionalfonts} % keeps proper math fonts

\usepackage{amsmath,amssymb,amsfonts} % math symbols (\mathcal, \mathbb, etc.)
\usepackage{mathrsfs}    
\usepackage{multicol}             % optional: \mathscr for fancy script

% \setbeamercovered{invisible} 
\setbeamercovered{transparent}


\title{MF921 Topics in Dynamic Asset Pricing}
\subtitle{Week 7}
\author{Yuanhui Zhao}
\date{Boston University}

\begin{document}
\frame{\titlepage}
% \begin{frame}
% \frametitle{Outline}
% \tableofcontents
% \end{frame}

\begin{frame}{Chapter 22}

    {
    \begin{center}
        Chapter 22 American Options (II)
    \end{center}
    \begin{center}
        Numerical Methods for American Options
    \end{center}
    }
    
\end{frame}

\begin{frame}{Recall}

    {\footnotesize \footnotesize
    Consider the valuation of a finite-horizon American option with payoff $\varphi(S(\tau))$:
    \[
    V(x,t) := \sup_{\tau \geq t} E^0 \left[ e^{-r(\tau - t)} \varphi(S(\tau)) \mid S(t) = x \right],
    \]
     where \( S(t) = \) stock price, \(\varphi(\cdot) = \) payoff function (e.g. \((K - S)^+\) for 
    a put), \(\tau = \) stopping time (the exercise time). So \( V(x,t) \) is the value function
    represent the highest discounted expected payoff you can achieve by optimally choosing when to stop.
    \vspace{1em}

     \pause First, the optimal reward and moving boundary functions together solve the
    following free-boundary (to be determined) problem:
        \[
        \begin{cases}
        \mathcal{A} V - rV = 0, & x > b(t), \quad 0 \leq t \leq T\\
        V(b(t), t) = \varphi(b(t)), & (\text{value matching}) \\
        V_x(b(t), t) = \varphi'(b(t)), & (\text{smooth pasting}) \\
        V(x, T) = \varphi(x), & x > 0 \\
        V(0, t) = \varphi(0), & V(\infty, t) = 0
        \end{cases}
        \]
    }
    
\end{frame}

\begin{frame}{Recall}

    {\footnotesize \footnotesize
    Second, the free boundary problem can be written as a partial differential complementarity problem (without the free boundary) as
    \begin{gather*}
        \mathcal{A} V - rV  \leq  0, \quad x > 0, \quad t \in [0,T) \\
    (\mathcal{A} V - rV) \{ V(x,t) - \varphi(x) \}  =  0, \quad x > 0, \quad t \in [0,T) \\
    V(x,t)  \geq \varphi(x), \quad x > 0, \quad t \in [0,T)
    \end{gather*}
    
    along with the terminal and boundary conditions

    \begin{gather*}
        V(x,T)  =  \varphi(x), \quad x > 0, \\
    \lim_{x \to \infty} V(x,t)  =  0, \quad V(0,t) = \varphi(0), \quad t \in [0,T)
    \end{gather*}

     \pause This is called the complementarity problem because the two inequalities cannot be strict 
    inequalities simultaneously. The partial differential complementarity problem can be solved 
    numerically by converting it to a matrix linear 
    complementarity problem by using the finite difference method.
        }
\end{frame}

\begin{frame}{Recall}

    {\footnotesize \footnotesize
    Third, the partial differential complementarity problem can be rewritten as a variational inequality problem:

    \[
    \min(-\mathcal{A} V + rV, V(x,t) - \varphi(x)) = 0, \quad x > 0, \quad t \in [0,T) \tag{*}
    \]

    along with the terminal and boundary conditions

    \[
    V(x,T) = \varphi(x), \quad x > 0,
    \]

    \[
    \lim_{x \to \infty} V(x,t) = 0, \quad V(0,t) = \varphi(0), \quad t \in [0,T).
    \]

     \pause Indeed, there are only two possibility by the partial differential complementarity 
    problem: either \(-\mathcal{A} V + rV > 0\) or \(-\mathcal{A} V + rV = 0\). In the first case we must 
    have \(V(x,t) = \varphi(x)\), and in the second case \(V(x,t) \geq \varphi(x)\). Thus, 
    in both cases we have $(*)$. Hence the partial differential complementarity problem 
    implies the variational inequality problem.  Conversely, $(*)$ implies that \(V(x,t) \geq \varphi(x)\) 
    and \(-\mathcal{A} V + rV \geq 0\). Furthermore, if \(-\mathcal{A}V + rV > 0\) then \(V(x,t) - \varphi(x) = 0\) by $(*)$. 
    \par Therefore, the variational inequality problem also implies the partial differential complementarity problem.
        }
\end{frame}

\begin{frame}{Recall}

    {\footnotesize \footnotesize
    An efficient way to solve the partial differential complementarity problem is
    to transform it ot a linear complementarity problem via the finite difference
    method.
    \vspace{1em}

    Consider the following (matrix) linear complementarity problem: 
    Find a vector \( x \in \mathbb{R}^{1 \times n} \)

    \[
    Ax \geq b, \, x \geq c, \, (x - c)^T (Ax - b) = 0,
    \]

    for given \( A \in \mathbb{R}^{n \times n}, \, c, b \in \mathbb{R}^{1 \times n} \), where, 
    for two column vectors \( x \) and \( y, \, x \geq y \) means \( x_i \geq y_i \) for each \( i \).
    The linear complementarity problem has a unique solution for all
     column vectors \( b \) and \( c \) if and only if \( A \in \mathbb{R}^{n \times n} \) is a P-matrix.
    \vspace{1em}
    
     \pause Note that if \( A \) is symmetric, then \( A \) is a P-matrix if and only if \( A \) is positive definite.
     Many matrices that arise in finite-difference and finite-element methods are diagonally dominant.
    }
    
\end{frame}
\begin{frame}{Recall}

    {\footnotesize \footnotesize
      More precisely, the matrix \( A \) is diagonally dominant if

    \[
    |a_{ii}| \geq \sum_{j \neq i} |a_{ij}|, \, \forall i,
    \]

    where \( a_{ij} \) denotes the entry of \( A \) in the \( i \)th row and \( j \)th column. 
    If the above inequality is strict, it is called strictly diagonally dominant. A well-known result is that a 
    symmetric strictly diagonally dominant matrix with real positive diagonal entries is positive definite.

    \vspace{1em}

     \pause The linear complementarity problem can be solved by using many methods, 
    including pivoting methods (e.g., Lemke's algorithm), quadratic programming, 
    successive over relaxation (SOR), projected SOR, etc. There are also several 
    Matlab and Python codes available online.

    }
    
\end{frame}


\begin{frame}{Recall}

    {\footnotesize \footnotesize
     The linear complementarity problem is a particular case of the nonlinear complementarity problem, which is to find a vector \( z \in \mathbb{R}^{1 \times n} \)

\[
f(x) \geq b, \, z \geq c, \, (z - c)^T(f(x) - b) = 0,
\]

where \( f \) is a given function \( \mathbb{R}^n \longmapsto \mathbb{R}^n, \, c, b \in \mathbb{R}^{1 \times n} \). 
One way to solve the nonlinear complementarity problem is to use the linear approximation of \( f(x) \) to get
an iterative algorithm by solving a sequence of the complementarity problem.

\vspace{1em}
 \pause The nonlinear complementarity problem is also related to a variational inequality problem: 
Given a non-empty set \( K \in \mathbb{R}^n \), a function $g: \mathbb{R}^n \longmapsto \mathbb{R}^n, \, b \in \mathbb{R}^{1 \times n},$
\(  \, \text{find a } 
z^* \in K \text{ such that} \)

\[
\min_{y \in K}(y - z^*)^T g(z^*) \geq 0.
\]

It can be shown that with

\[
K = \mathbb{R}_+^{1 \times n}, \, z^* = z - c, \, g(x) = f(x) - b,
\]

the nonlinear complementarity problem and the variational inequality problem have the same solution.

    }
    
\end{frame}


\begin{frame}{Finite Difference Methods}

    {\footnotesize \footnotesize
    The partial differential complementarity problem for \(\psi(x,t)\) without the free boundary,

    \[
    \frac{\partial \psi}{\partial t} + \frac{\sigma^2 x^2}{2} \frac{\partial^2 \psi}{\partial x^2} + rx \frac{\partial \psi}{\partial x} - r\psi \leq 0
    \]

    \[
    \left( \frac{\partial \psi}{\partial t} + \frac{\sigma^2 x^2}{2} \frac{\partial^2 \psi}{\partial x^2} + rx \frac{\partial \psi}{\partial x} - r\psi \right) \{ \psi(t,x) - g(x) \} = 0
    \]

    \[
    \psi(x,t) \geq g(x)
    \]

    \[
    \psi(x,T) = g(x)
    \]

    \[
    \lim_{x \to \infty} \psi(x,t) = 0, \quad \psi(0,t) = g(0),
    \]

    \pause Where $g(x)$ is the payoff function. Although the algorithm also works for the case where $g(x)$ may be $g(x,t)$ depending on $t$,
    for notational simplicity we shall focus on the case of time-independent $g(x)$. 
    First, we can transform the problem based on the standard heat equation by changing variables. 

        }
    
\end{frame}

\begin{frame}{Finite Difference Methods}

    {\footnotesize \footnotesize
    \vspace{-2em}
    \begin{gather*}
        S = e^x, \quad \tau = (T - t) \frac{\sigma^2}{2}\\
        u(x,\tau) = \exp\left\{ \frac{1}{2} (c - 1)x +
         \frac{1}{4} (c + 1)^2 \tau \right\} \cdot \psi(S,t), \quad c = \frac{2r}{\sigma^2},
    \end{gather*}
    
        the Black-Scholes partial differential complementarity problem becomes 
        a standard heat partial differential complementarity problem, $x \in (-\infty, \infty)$:
         \pause \begin{gather*}
            \frac{\partial u}{\partial \tau} - \frac{\partial^2 u}{\partial x^2} \geq 0\\
            \left( \frac{\partial u}{\partial \tau} - \frac{\partial^2 u}{\partial x^2} \right) \{u(\tau, x) - f(x)\} = 0\\
            u(x, \tau) \geq f(x, \tau), \quad u(x, 0) = f(x, 0) \\
            \lim_{x \to \infty} u(x, \tau) = 0
        \end{gather*}
        where $f(x, \tau) = \exp \left\{ \frac{1}{2} (c-1)x + \frac{1}{4} (c+1)^2 \tau \right\} g(e^x)$.
        
        This is the heat equation complementarity problem, with constant coefficients. 
        We are interested in finding $u(x, \tau)$, where
        $x \in (-\infty, \infty)$ and $\tau \in [0, T\sigma^2/2]$.
        }
    
\end{frame}

\begin{frame}{Finite Difference Methods}

    {\footnotesize \footnotesize
    We have the transformed the complicated Black-Scholes complementarity PDE
    into  heat-equation complementarity problem:
    \[
    u_\tau - u_{xx} \geq 0, \quad (u_\tau - u_{xx})(u - f) = 0, \quad u \geq f.
    \]
    \pause  We can't solve this analytically,
    so we approximate the derivatives $(u_\tau, u_{xx})$ with finite differences on a discrete grid.
    By discretizing time and space on a regular mesh with step sizes $\delta \tau$ and $\delta x$, 
    and truncating $x$ 
    in the region $[-N^- \delta x, N^+ \delta x]$ for suitably large integers $N^+$ and $N^-$, we let
    \begin{align*}
    u_{n,m} = u(n\delta x, m\delta \tau), \quad f_{n,m} = f(n\delta x, m\delta \tau), \quad
    -N^- \leq n \leq N^+, \quad 0 \leq m \leq M,
    \end{align*}
    where $M\delta \tau = T\sigma^2/2$.
    \par  \pause We use the finite difference approximation
    {\footnotesize \tiny
    \begin{align*}
    \frac{\partial u}{\partial \tau} &= \frac{u_{n,m+1} - u_{n,m}}{\delta \tau} + o(\delta \tau), \\
    \frac{\partial^2 u}{\partial x^2} &= \theta \left( \frac{u_{n+1,m+1} - 2u_{n,m+1} + u_{n-1,m+1}}{(\delta x)^2} \right) \\
    &\quad + (1 - \theta) \left( \frac{u_{n+1,m} - 2u_{n,m} + u_{n-1,m}}{(\delta x)^2} \right) + o((\delta x)^2),
    \end{align*}
    }
    where $0 < \theta < 1$.  
        }
    
    
\end{frame}

\begin{frame}{Finite Difference Methods}

    {\footnotesize \footnotesize

    Note that this is a general way to do the finite difference method;
     the cases $\theta = 0, 1/2, 1$ yield the explicit, the Crank-Nicolson, and the implicit schemes, respectively.
    \vspace{1em}

    The explicit scheme does not involve solving linear equations and is known
    to be numerically stable and convergent whenever
    \[
    \alpha := \delta \tau / (\delta x)^2 \leq 1/2.
    \]

     \pause The implicit and the Crank-Nicolson schemes are always numerically stable and convergent 
    but more numerically intensive than the explicit method. They require solving a system of 
    linear equations on each time step. Usually, the Crank-Nicolson scheme is the most accurate 
    scheme for small time steps, while the implicit scheme works best for large time steps. In general, 
    if $\theta < 1/2$, then the scheme is stable if $\delta \tau / (\delta x)^2 < (1 - \theta)/2$.
    }
    
    
\end{frame}

\begin{frame}{Finite Difference Methods}

    {\footnotesize \footnotesize
    With these approximations, we have
    {\footnotesize \tiny
    \begin{align*}
    \frac{\partial u}{\partial \tau} - \frac{\partial^2 u}{\partial x^2} \approx & \frac{u_{n,m+1} - u_{n,m}}{\delta \tau} - \theta \left( \frac{u_{n+1,m+1} - 2u_{n,m+1} + u_{n-1,m+1}}{(\delta x)^2} \right) \\
    & - (1 - \theta) \left( \frac{u_{n+1,m} - 2u_{n,m} + u_{n-1,m}}{(\delta x)^2} \right).
    \end{align*}
        }

     \pause Thus, the requirement that $\frac{\partial u}{\partial \tau} - \frac{\partial^2 u}{\partial x^2} \geq 0$ becomes for $-N^- + 2 \leq n \leq N^+ - 2$, $0 \leq m \leq M - 1$,
    \[
    u_{n,m+1} - \alpha \theta (u_{n+1,m+1} - 2u_{n,m+1} + u_{n-1,m+1}) \geq b_{n,m}, 
    \]
    where
    \[
    b_{n,m} := u_{n,m} + \alpha (1 - \theta) (u_{n+1,m} - 2u_{n,m} + u_{n-1,m}),
    \quad \alpha = \delta\tau/(\delta x)^2.
    \]
     Note that if we move forward in time, at the $(m + 1)$-th step we
     know $b_{n,m}$ explicitly, via $u_{n,m}$, $u_{n+1,m}$, and $u_{n-1,m}$.

    }
    
    
\end{frame}

\begin{frame}{Finite Difference Methods}

    {\footnotesize \footnotesize
     The requirement $u(\tau, x) - f(\tau, x) \geq 0$ becomes, for $-N^- \leq n \leq N^+$, $0 \leq m \leq M$,
    \[
    u_{n,m} \geq f_{n,m}. 
    \]

    To simplify the operation, we shall translate the complementarity condition to $(m + 1)$-th time point as, for $-N^- + 2 \leq n \leq N^+ - 2$, $0 \leq m \leq M - 1$,
    \[
    \{u_{n,m+1} - \alpha \theta (u_{n+1,m+1} - 2u_{n,m+1} + u_{n-1,m+1}) - b_{n,m}\} \{u_{n,m+1} - f_{n,m+1}\} = 0.
    \]

     \pause The initial condition is easy because $u(0, x) = f(0, x)$, yielding
    \[
    u_{n, 0} = f_{n, 0}. 
    \]

    We have to first pay attention to the boundary conditions at $-N^{-}$ and $N^{+}$. 
    Based on the definition of option payoffs, as an approximation we can let
    \[
    u_{-N^{-},m} = f(-N^{-},m), \quad u_{N^{+},m} = f(N^{+},m), \quad 0 \leq m \leq M. 
    \]
     \pause  The intuition is that when stock price $S(t) \to \infty$ or $S(t) \to 0$, 
    corresponding to $x \to \infty$ and $x \to -\infty$, 
    the option price at time $t$ should be very close to the option payoff at time $t$.
    }
    
    
\end{frame}

\begin{frame}{The Boundary Conditions and the Initial Condition}

    {\footnotesize \footnotesize
    The discretized complementarity PDE for the transformed variable $u(x,\tau)$ only works for interior points
    $n$ where both neighbors $u_{n+1}$ and $u_{n-1}$ exist. 
    At the boundaries, one neighbor lies outside the computational grid.
    \vspace{1em}

     \pause For the term $u_{n,m+1}$ when $n = N^{+} - 1$, we have
    \begin{align*}
    &u_{N^{+}-1,m+1} - u_{N^{+}-1,m} - \alpha \theta (u_{N^{+},m+1} - 2u_{N^{+}-1,m+1} + u_{N^{+}-2,m+1}) \\
    &\geq \alpha(1 - \theta) (u_{N^{+},m} - 2u_{N^{+}-1,m} + u_{N^{+}-2,m}), 
    \end{align*}
    for $0 \leq m \leq M - 1$, or
    \[
    u_{N^{+}-1,m+1} - \alpha \theta (-2u_{N^{+}-1,m+1} + u_{N^{+}-2,m+1}) \geq b_{N^{+}-1,m} + 
    \alpha \theta u_{N^{+},m+1},
    \]

     \pause where
    \[
    b_{N^{+}-1,m} := u_{N^{+}-1,m} + \alpha (1 - \theta) (u_{N^{+},m} - 2u_{N^{+}-1,m} + u_{N^{+}-2,m}),
    \]
    \[
    u_{-N^{-},m+1} = f(-N^{-},m+1), \quad 0 \leq m \leq M - 1.
    \]
    }
    
    
\end{frame}

\begin{frame}{The Boundary Conditions and the Initial Condition}

    {\footnotesize \footnotesize
    Similarly for the term $u_{n,m+1}$ when $n = -N^{-} + 1$, we have
    \begin{align*}
    &u_{-N^{-}+1,m+1} - u_{-N^{-}+1,m} - \alpha \theta (u_{-N^{-}+2,m+1} - 2u_{-N^{-}+1,m+1} + u_{-N^{-},m+1}) \\
    &\geq \alpha (1 - \theta) (u_{-N^{-}+2,m} - 2u_{-N^{-}+1,m} + u_{-N^{-}+1,m}),
    \end{align*}
    for $0 \leq m \leq M - 1$, or
    \[
    u_{-N^{-}+1,m+1} - \alpha \theta (u_{-N^{-}+2,m+1} - 2u_{-N^{-}+1,m+1}) 
    \geq b_{-N^{-}+1,m} + \alpha \theta u_{-N^{-},m+1}, 
    \]
     \pause where
    \[
    b_{-N^{-}+1,m} := u_{-N^{-}+1,m} + \alpha (1 - \theta) (u_{-N^{-}+2,m} - 2u_{-N^{-}+1,m} + u_{-N^{-}+1,m}).
    \]

    In summary we can define
    \[
    b_{n,m} := u_{n,m} + \alpha (1 - \theta) (u_{n+1,m} - 2u_{n,m} + u_{n-1,m}),
    \quad -N^{-} + 1 \leq n \leq N^{+} - 1. 
    \]

    The linear complementarity condition at the two boundaries can be written down readily.

    }
    
    
\end{frame}

\begin{frame}{Standard-Form of the Linear Complementarity Problem}

    {\footnotesize \footnotesize
    Introduce the column vectors
    \begin{gather*}
        u^{(m)} := (u_{-N^-+1,m}, \ldots, u_{N^+-1,m})^T, \\
        f^{(m)} := (f_{-N^-+1,m}, \ldots, f_{N^+-1,m})^T,\\
        b^{(m)} := (b_{-N^-+1,m}, \ldots, b_{N^+-1,m})^T,\\
    \end{gather*}
    \vspace{-3em}
    \par  \pause So each vector represents one time slice of all the interior spatial nodes. At the boundaries,
    we define a correction vector to represent how the known boundary 
    values affect the equations for the first and last interior points.
    \begin{align*}
    \xi^{(m)} &:= (\alpha \theta u_{-N^-,m+1}, 0, 0, \ldots, 0, 0, \alpha \theta u_{N^+,m+1})^T,
    \end{align*}
     \pause and a $(N^+ + N^- - 2) \times (N^+ + N^- - 2)$ square, tridiagonal, symmetric matrix
    \[
    C = 
    \begin{pmatrix}
    1 + 2\alpha\theta & -\alpha\theta & 0 & \cdots & 0 \\
    -\alpha\theta & 1 + 2\alpha\theta & -\alpha\theta & \cdots & 0 \\
    0 & -\alpha\theta & \ddots & \ddots & \vdots \\
    \vdots & & \ddots & 1 + 2\alpha\theta & -\alpha\theta \\
    0 & 0 & \cdots & -\alpha\theta & 1 + 2\alpha\theta
    \end{pmatrix}.
    \]

    }
    
    
\end{frame}

\begin{frame}{Standard-Form of the Linear Complementarity Problem}

    {\footnotesize \footnotesize
    Then the inequalities:
    \begin{gather*}
        u_{n,m+1} - \alpha \theta (u_{n+1,m+1} - 2u_{n,m+1} + u_{n-1,m+1}) \geq b_{n,m}\\
        u_{N^{+}-1,m+1} - \alpha \theta (-2u_{N^{+}-1,m+1} + u_{N^{+}-2,m+1}) 
        \geq b_{N^{+}-1,m} + \alpha \theta u_{N^{+},m+1}\\
        u_{-N^{-}+1,m+1} - \alpha \theta (u_{-N^{-}+2,m+1} -
         2u_{-N^{-}+1,m+1}) \geq b_{-N^{-}+1,m} + \alpha \theta u_{-N^{-},m+1},
    \end{gather*}
    become to:
    \begin{align*}
        Cu^{(m+1)} \geq b^{(m)} + \xi^{(m)}.
    \end{align*}
    \par  \pause The inequality $u_{n,m} \geq f_{n,m}$ can be written as $u^{(m+1)} \geq f^{(m+1)}$. 
    The linear complementarity condition becomes
    \begin{align*}
        \left(u^{(m+1)} - f^{(m+1)}\right)^T \left(Cu^{(m+1)} - b^{(m)} - \xi^{(m)}\right) = 0.
    \end{align*}
    }
    
    
\end{frame}
\begin{frame}{Standard-Form of the Linear Complementarity Problem}

    {\footnotesize \footnotesize
     In summary, the algorithm goes from $m = 0$ to $m = M - 1$, as follows. 

    \textbf{Initialization:} At maturity ($m = 0$):
    \[
    u^{(0)} = f^{(0)} = \text{payoff}.
    \]

     \pause \textbf{Backward time-stepping (for $m = 0, 1, \ldots, M - 1$):}
    \begin{enumerate}
    \item \textbf{Compute known quantities:}
    \[
    b^{(m)} = u^{(m)} + \alpha(1 - \theta)(\text{Laplacian of } u^{(m)}),
    \]
    %(u_{n+1,m} - 2u_{n,m} + u_{n-1,m})
    and boundary correction $\xi^{(m)}$.

     \pause \item \textbf{Set up right-hand side:}
    \[
    q = b^{(m)} + \xi^{(m)}.
    \]

     \pause \item \textbf{Solve LCP:}
    \[
    \begin{cases}
    C u^{(m+1)} \geq q, \\
    u^{(m+1)} \geq f^{(m+1)}, \\
    (u^{(m+1)} - f^{(m+1)})^{T}(C u^{(m+1)} - q) = 0.
    \end{cases}
    \]
    \par  Use Projected Successive Over-Relaxation (or another solver) to find $u^{(m+1)}$.

    \item \textbf{Repeat until $m = M - 1$.}
    \end{enumerate}
    

    }
    
    
\end{frame}
\begin{frame}{Standard-Form of the Linear Complementarity Problem}

    {\footnotesize \footnotesize
    Note that since
    \[
    |1 + 2\alpha\theta| = 1 + 2\alpha\theta > |-\alpha\theta| + |-\alpha\theta|,
    \]
    the symmetric matrix $C$ is a strictly diagonally dominant matrix with real positive diagonal entries. 
    Thus, the symmetric matrix $C$ is positive definite, and the linear complementarity problem 
    given by three inequalities has a unique solution.
    \vspace{1em}
    \par  \pause Projected Successive Over-Relaxation (PSOR)
    \par We could solve that exactly by inversion, but for 
    large systems (like in PDE grids), we often use iterative methods.
    \par We rearrange each equation for its main variable $u_i$:
\[
C_{ii}u_i = q_i - \sum_{j\neq i} C_{ij}u_j.
\]

Then we update $u_i$:
\[
u_i = \frac{1}{C_{ii}} \left( q_i - \sum_{j\neq i} C_{ij}u_j \right).
\]

That's the Gauss-Seidel update rule.

    }
    
    
\end{frame}
 


\begin{frame}{Standard-Form of the Linear Complementarity Problem}

    {\footnotesize \footnotesize
     For our tridiagonal $C$, each row has only three nonzero entries:
    \[
    -\alpha \theta u_{i-1} + (1 + 2\alpha \theta)u_i - \alpha \theta u_{i+1} = q_i.
    \]

    Rearrange for $u_i$:
    \[
    u_i = \frac{1}{1 + 2\alpha \theta} \left( q_i + \alpha \theta u_{i-1} + \alpha \theta u_{i+1} \right).
    \]
    
    \pause  We also need $u_i \geq f_i$, we simply project each update:
    \[
    u_i^{(k+1)} = \max \left( f_i, \frac{1}{C_{ii}} \left[ q_i - \sum_{j<i} C_{ij} u_j^{(k+1)} 
    - \sum_{j>i} C_{ij} u_j^{(k)} \right] \right).
    \]
    \vspace{1em}
    \par After repeat the algorithm until $m = M-1$, we hold a vector $u^{(M)}$ :
    \begin{align*}
        u^{(M)} = [u_{-N^- + 1, M}, \ldots, u_{N^+ - 1, M}]^T.
    \end{align*}
    That vector contains the transformed values of 
    the option at time $t = 0$ (the valuation date) for different $x = \ln S$ grid points.
    }
\end{frame}


\begin{frame}{Standard-Form of the Linear Complementarity Problem}

    {\footnotesize \footnotesize
     So each component corresponds to:
    \[
    u_{n, M} \approx u(x_n, \tau_{\max}) = e^{\frac{1}{2}(c-1)x_n + \frac{1}{4}(c+1)^2 \tau_{\max}} \psi(S_n, 0),
    \]
    where $S_n = e^{x_n}$.

        To return to the original (Black-Scholes) variables, recall the transformation:
    \[
    u(x, \tau) = e^{\frac{1}{2}(c-1)x + \frac{1}{4}(c+1)^2 \tau} \psi(S, t), \quad c = \frac{2r}{\sigma^2}.
    \]

    At $t = 0$, $\tau = T\sigma^2 / 2$.

     \pause So the original option price surface is:
    \[
    \psi(S, 0) = e^{-\frac{1}{2}(c-1)\ln S - \frac{1}{4}(c+1)^2 \tau_{\max}} u(\ln S, \tau_{\max}).
    \]

    That gives the American option price at time 0 for each stock price $S$ on the grid.
    Usually we're interested in the price for one current stock price $S_0$. If $S_0$ 
    lies between two grid nodes $S_i, S_{i+1}$, we interpolate linearly to find $\psi(S_0, 0)$.
    We can also track at each time step where $u_{n,m} = f_{n,m}$ (the points where equality binds) to
     reconstruct the exercise boundary $S^*(t)$.
    }
\end{frame}


\begin{frame}{Chapter 10}

    {
    \begin{center}
        Chapter 10 Arbitrage
    \end{center}
 
    }
    
\end{frame}
\begin{frame}{Motivation to Study Arbitrage}

    {\footnotesize \footnotesize
    \textbf{Can there be arbitrage even without options?}
    \vspace{1em}
    \par If your model of stock price dynamics itself creates a riskless profit (without any options), 
    then the model is internally inconsistent.  If the relationship between the interest rate \( r \)
    and the possible stock movements (up and down) is inconsistent, arbitrage automatically arises.
    \vspace{1em}
    \par A valid model (arbitrage-free) must satisfy:
    \[
    d < R < u,
    \]

     \pause where:
    \begin{itemize}
    \item \( u = \text{up-factor} = S_u / S_0 \),
    \item \( d = \text{down-factor} = S_d / S_0 \),
    \item \( R = 1 + r \).
    \end{itemize}

    Only in that range can there exist a risk-neutral probability \( p^* = \frac{R-d}{u-d} \) with \( 0 < p^* < 1 \).
    Arbitrage-free condition ensures no riskless trading profit 
    can be made by combining borrowing/lending and buying/selling the stock.
    If your model violates this, then the entire pricing framework collapses.

    }
\end{frame}

\begin{frame}{Basic Setting of a One-Period Security Market}

    {\footnotesize \footnotesize
     We construct the simplest possible financial market to study arbitrage formally.
     \begin{itemize}
        \item The model is discrete in time, \( t = 0 \) and \( t = 1 \), and finite in states (K scenarios), 
        \( \Omega = \{\omega_1, \ldots, \omega_K\} \), \( P(\omega) > 0 \).
        \item \( N + 1 \) securities: \( B(t) \) is bank account (risk-free), \( B(0) = 1 \), \( B(1) = 1 + r \) and 
        \( S_1(t), S_2(t), \ldots, S_N(t) \) are risky stocks.
        \item Trading Strategy: \( \varphi = (\varphi_0, \varphi_1, \ldots, \varphi_N) \) are portfolio weights 
        and \( V(0) \) is initial wealth. The resulting wealth process is given by
        \begin{align*}
            V(t) := \varphi_0 B(t) + \sum_{n=1}^N \varphi_n S_n(t) .
        \end{align*}
     \end{itemize}
      \par  \pause The self-financing condition is defined as:
        \begin{align*}
            V(1) - V(0) = \phi_0 \Delta B + \sum_{n=1}^N \phi_n \Delta S_n,
        \end{align*}
        where $\Delta B := B(1) - B(0)=r, \quad \Delta S_n := S_n(1) - S_n(0).$

    }
\end{frame}

\begin{frame}{Basic Setting of a One-Period Security Market}

    {\footnotesize \footnotesize
    Define discounted variables:
    \begin{align*}
        S_n^*(t) := S_n(t)/B(t), \quad V^*(t) := V(t)/B(t) = \phi_0 + \sum_{n=1}^N \phi_n S_n^*(t) \quad t = 0, 1,
    \end{align*}
    then we can rewrite the self-financing condition as
    \begin{align*}
        V^*(1) - V^*(0) = \sum_{n=1}^N \phi_n \Delta S_n^*, \quad V^*(t) = V(t)/B(t).
    \end{align*}
     \pause Define the capital gain process
    \begin{align*}
        G^* = \sum_{n=1}^N \phi_n \Delta S_n^*, \quad  V^*(1) = V(0) + G^*.
    \end{align*}

    \par The simple return of \( n \)th risky asset is defined as $R_n := (S_n(1) - S_n(0))\ S_n(0).$
    }
\end{frame}

\begin{frame}{Dominant Trading Strategies}

    {\footnotesize \footnotesize
    
    A trading strategy \(\phi\) is said to dominate another strategy \(\tilde{\phi}\) if their corresponding wealth \(V\) and \(\tilde{V}\) satisfy:
    %
    \[
    V(0) = \tilde{V}(0),\quad V(1) > \tilde{V}(1),
    \]
    %
    with probability one (i.e.\ \(V(1, \omega) > \tilde{V}(1, \omega)\) for any \(\omega \in \Omega\)).
    \vspace{1em}
    \par  \pause \textbf{Lemma 1.} The following statements are equivalent.
    \begin{itemize}
        \item There is a dominant strategy.
        \item There exists a trading strategy such that \(V(0) = 0\) and \(V(1) > 0\) with probability one.
        \item There exists a trading strategy such that \(V(0) < 0\) and \(V(1) \geq 0\) with probability one.
    \end{itemize}
    \par \pause  \textbf{Proof}:
    \par (a) \(\Rightarrow\) (b): Suppose there are two trading strategies \(\hat{\phi}\) and \(\tilde{\phi}\) such that \(\hat{\phi}\) dominates \(\tilde{\phi}\). Then consider another trading strategy \(\phi = \hat{\phi} - \tilde{\phi}\). We have, via the definition of wealth processes,
    %
    \[
    V(0) = \hat{V}(0) - \tilde{V}(0) = 0,\quad V(1) = \hat{V}(1) - \tilde{V}(1) > 0,
    \]
    %
    with probability one, from which (b) follows.
    }
\end{frame}


\begin{frame}{Dominant Trading Strategies}

    {\footnotesize \footnotesize
    \par  (b) \(\Rightarrow\) (c): Now start from (b): \( V(0) = 0 \), \( V(1) > 0 \) in all states.
    Since \( V^*(1) = G^* > 0 \) for all states, the smallest possible gain $\varepsilon = \min_{\omega \in \Omega} G^*(\omega) > 0$
    exists. Construct a new strategy:
    \vspace{-1em}
    \begin{align*}
        \tilde{\phi}_n = \phi_n, \quad \tilde{\phi}_0 = \phi_0 - \varepsilon.
    \end{align*}
    That reduces the initial wealth by \(\varepsilon\):
    \begin{align*}
        \tilde{V}(0) = -\varepsilon < 0, \quad \tilde{V}^*(1, \omega) = -\varepsilon + G^*(\omega) \geq 0.
    \end{align*}
    \par So you borrow money at the beginning (\(V(0) < 0\)), but never lose at the end (\(V(1) \geq 0\)). That's (c).
    \par  \pause (c) $\Rightarrow$ (a): Now suppose we have (c): \( V(0) < 0 \) and \( V(1) \geq 0 \). 
    Construct a new strategy \(\tilde{\phi}\):
    \vspace{-1em}
    \begin{align*}
        \tilde{\phi}_n = \phi_n, \quad \tilde{\phi}_0 = -\sum_{n=1}^{N} \phi_n S_n(0) = \phi_0 - V(0).
    \end{align*}
    \vspace{-1em}
    \par Then the new portfolio has:
%
\[
\tilde{V}(0) = 0, \quad \tilde{V}^*(1) = -V(0) + V^*(1) \geq -V(0) > 0.
\]
\par So \(\tilde{\phi}\) strictly dominates the ``do nothing'' strategy (holding nothing), satisfying (a).
    }
\end{frame}

\begin{frame}{Law of one price}

    {\footnotesize \footnotesize
    The law says that there should not exist two trading strategies $\phi$ and $\phi'$ such that  
    \[
    V(0) > V'(0), \quad V(1, \omega) = V'(1, \omega), \quad \forall \omega \in \Omega.
    \]
    \par \textbf{Lemma}: No dominant strategies implies the law of one price. In other words, 
    the law of one price is a weaker assumption than the assumption of no dominant strategies.
    \par  \pause \textbf{Proof}
    \par We shall prove this by contradiction. Suppose the law of one price is violated. Then there are two strategies $\phi$ and $\phi'$ such that  
\[
V(0) > V'(0), \quad V(1, \omega) = V'(1, \omega), \quad \forall \omega \in \Omega.
\]  
Now consider a new strategy $\tilde{\phi} = \phi' - \phi$. Then  
\[
\tilde{V}(0) = V'(0) - V(0) < 0, \quad \tilde{V}(1, \omega) = V'(1, \omega) - V(1, \omega) = 0 \geq 0.
\]  
Therefore, part (c) in the equivalence of dominant strategies is satisfied, and there exists dominant strategies. A contradiction.
\par \pause  Note: Law of one price not imply no dominant strategies.
    }
\end{frame}

\begin{frame}{Arbitrage}

    {\footnotesize \footnotesize
     An arbitrage opportunity arises when there exists a trading strategy such that:
     \begin{align*}
        V(0)=0,\quad V(1)\geq 0,\quad \mathbb{E}[V(1)]>0.
     \end{align*}
    If there exists a dominant strategy then there must be an arbitrage opportunity. 
     However, the converse is not true.
     \par  \pause \textbf{Proposition}: No arbitrage opportunities $\Rightarrow$ no 
     dominant strategies $\Rightarrow$ law of one price.
     \par \textbf{Lemma}: The following three statements are equivalent.
     \begin{enumerate}
        \item[(a)] $\phi$ is an arbitrage opportunity
        \item[(b)] The discounted wealth $V^*$ associated with $\phi$ satisfies $V^*(0) = 0$, $V^*(1) \geq 0$, and $\mathbb{E}(V^*(1)) > 0$;
        \item[(c)] The associated capital gain process $G^*$ satisfies $G^* \geq 0$ and $\mathbb{E}[G^*] > 0$.
        \end{enumerate}
    \par  \pause \textbf{Proof}:
    First of all, (a) and (b) are equivalent simply because $B(1) > 0$.

Next, consider statement (c). Suppose $\phi$ is an arbitrage opportunity. We have from (b)
\[
G^* = V^*(1) - V(0) = V^*(1) \geq 0,
\]
and
\[
\mathbb{E}[G^*] = \mathbb{E}[V^*(1)] > 0.
\]
    }
\end{frame}


\begin{frame}{Arbitrage}

    {\footnotesize \footnotesize
        Conversely, suppose $G^* \geq 0$ and $\mathbb{E}[G^*] > 0$ for some strategy $\phi$. Introduce a new strategy $\tilde{\phi} = (\tilde{\phi}_0, \phi_1, \ldots, \phi_N)$, where for the saving account $\tilde{\phi}_0 = -\sum_{n=1}^N \phi_n S_n^*(0)$. Then we see
    \[
    \tilde{V}^*(0) = \tilde{\phi}_0 + \sum_{n=1}^N \phi_n S_n^*(0) = 0,
    \]
    and
    \[
    \tilde{V}^*(1) = \tilde{\phi}_0 + \sum_{n=1}^{N} \phi_n S_n^*(1) = \tilde{\phi}_0 + \sum_{n=1}^{N} \phi_n S_n^*(0) + G^* = G^*.
    \]

     \pause Thus,
    \[
    \tilde{V}^*(0) = 0, \quad \tilde{V}^*(1) \geq 0, \quad \mathbb{E}[\tilde{V}^*(1)] > 0,
    \]
    which is an arbitrage opportunity. Therefore, we finish the proof of the equivalence of (a), (b), (c).
    }
\end{frame}

\begin{frame}{Linear Pricing Measure}

    {\footnotesize \footnotesize
      A linear pricing measure is a non-negative vector $(\pi(\omega_1), \ldots, \pi(\omega_K))$ 
      such that for every trading strategy $\phi$, we must have
    \[
    V(0) = \sum_{j=1}^K \pi(\omega_j) V^*(1, \omega_j),
    \]
    where recall that $\Omega = (\omega_1, \ldots, \omega_K)$. 
    Note that we only know that $\pi \geq 0$. But some of the $\pi(\omega_i)$ could be zero.
    \par  \pause \textbf{Lemma}:
    \par We must have $\sum_{j=1}^K \pi(\omega_j) = 1$. Thus, $\pi$ must be a probability measure, and for every trading strategy $\phi$
    \[
    V(0) = E_\pi [V^*(1)].
    \]

    In other words, if there exists a linear pricing measure, 
    then the measure must be a probability measure, and the price of any asset at time 0 can be written as the expectation of the discounted final 
    payoff of the asset, where the expectation is taken with respect to the measure $\pi$.
    }
\end{frame}
\begin{frame}{Linear Pricing Measure}

    {\footnotesize \footnotesize
       \par \textbf{Proof}:
        \par Consider a strategy $\phi$ with $\phi_1 = \phi_2 = \cdots = \phi_N = 0$. In other words, the strategy only invests in the saving account and not in any stocks. Then $V^*(1) \equiv V(0)$.
    \[
    V(0) = \sum_{j=1}^K \pi(\omega_j) V^*(1) = \sum_{j=1}^K \pi(\omega_j) V(0),
    \]
    from which $\sum_{j=1}^K \pi(\omega_j) = 1$, and the conclusion follows.
    \vspace{1em}

    \par  \pause \textbf{Theorem 1}: There are no dominant strategies 
    if and only if there exists a linear pricing (probability) measure.
    \vspace{1em}
    \par Proof later.
    }
\end{frame}


\begin{frame}{Risk-Neutral Measure}

    {\footnotesize \footnotesize
        A probability measure $Q$ is called a risk-neutral measure if $Q$ satisfies two requirements:
    \begin{enumerate}
    \item[(i)] $Q(\omega_i) > 0$, for all $i = 1, \ldots, K$.
    \item[(ii)] $E_Q(S_n^*(1)) = S_n(0)$ for all $n = 1, \ldots, N$.
    \end{enumerate}

    Under a risk-neutral measure, every asset has the same return as that 
    of the saving account. Therefore, under this measure, 
    investor does not give any higher return for higher risk, thus risk-neutral.
    \vspace{1em}
    \par  \pause Note that in terms of the simple return, under the risk neutral probability $Q$, we must have
    \[
    E_Q[R_n] = r, \quad R_n = \frac{S_n(1) - S_n(0)}{S_n(0)}.
    \]
    because starting from the discounted-price condition (ii):  
    \[
    E_Q[S_n^*(1)] = S_n(0), \quad S_n^*(1) = \frac{S_n(1)}{B(1)}.
    \]

    Since $B(1) = 1 + r$,  
    \begin{align*}
    E_Q\left[\frac{S_n(1)}{1 + r}\right] = S_n(0) 
    \Rightarrow E_Q[S_n(1)] = S_n(0)(1 + r) 
    \Rightarrow E_Q\left[\frac{S_n(1) - S_n(0)}{S_n(0)}\right] = r.
    \end{align*}

    Thus $E_Q[R_n] = r$.

    }
\end{frame}

\begin{frame}{Risk-Neutral Measure}

    {\footnotesize \footnotesize
    \par A risk neutral probability measure $Q$ is not the real probability 
    measure $P$ observed in markets! 
    However, it is a very important mathematical tool to derive pricing formulae. 
    \vspace{1em}
    \par  \pause \textbf{Theorem 2}: Absence of arbitrage opportunity is equivalent 
    to the existence of a risk-neutral measure.
    \vspace{1em}
    \par  \pause To prove Theorem 2, we shall show first that the existence of a risk neutral measure guarantees no arbitrage opportunity. Suppose there exist a risk-neutral measure $Q$. Then for any trading strategy $\phi$, we must have
    \begin{align*}
    E_Q[G^*] &= E_Q\left[\sum_{n=1}^N \phi_n \Delta S_n^*\right] = \sum_{n=1}^N \phi_n E_Q[\Delta S_n^*] 
    = \sum_{n=1}^N \phi_n E_Q[S_n^*(1) - S_n(0)] = 0,
    \end{align*}
     \pause by part (ii) in the definition of risk-neutral measure. Now suppose there is an arbitrage opportunity. Then there is a trading strategy $\phi$ such that
    \[
    G^* \geq 0, \quad E[G^*] = \sum_{j=1}^K G^*(\omega_j) P(\omega_j) > 0,
    \]
    }
\end{frame}

\begin{frame}{Risk-Neutral Measure}

    {\footnotesize \footnotesize
        by Part (c) in the equivalent statements of arbitrage. Thus, there must be a $G^*(\omega_k) > 0$ and $G^*(\omega_j) \geq 0$ for all $j = 1, \ldots, K$. Therefore,
    \[
    E_Q[G^*] = \sum_{j=1}^K G^*(\omega_j) Q(\omega_j) \geq G^*(\omega_k) Q(\omega_k) > 0,
    \]
    as $Q(\omega_k) > 0$. A contradiction to the fact that $E_Q[G^*] = 0$. The proof is terminated.

    The harder part is to show that no arbitrage 
    implies the existence of a risk neutral measure. The proof will give later.
    \vspace{1em}

    \par  \pause \textbf{Corollary:} In the binomial tree model there is no arbitrage 
    for trading stocks if and only if $u > R > d$.
    \vspace{1em}
    \par \textbf{Proof}: This is because there is a
    \[
    p^* = \frac{R - d}{u - d},
    \]
    and $p^* \in (0, 1)$ if and only if $u > R > d$.
    }
\end{frame}

\begin{frame}{Linear Programming and Linear Pricing Measures}

    {\footnotesize \footnotesize
     There are two optimization problems in linear programming:

    \begin{multicols}{2}
\noindent
(1) Primal Problem
\[
\begin{aligned}
&\max_{x_1,\ldots,x_n} c_1 x_1 + c_2 x_2 + \cdots + c_n x_n \\
&\text{subject to: } \\
&a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \leq b_1 \\
&a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \leq b_2 \\
&\vdots \\
&a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n \leq b_m \\
&x_j \geq 0, \quad j = 1, \ldots, n
\end{aligned}
\]

% \columnbreak

\noindent
 \pause (2) Dual Problem
\[
\begin{aligned}
&\min_{y_1,\ldots,y_m} b_1 y_1 + b_2 y_2 + \cdots + b_m y_m \\
&\text{subject to: } \\
&a_{11} y_1 + a_{21} y_2 + \cdots + a_{m1} y_m \geq c_1 \\
&a_{12} y_1 + a_{22} y_2 + \cdots + a_{m2} y_m \geq c_2 \\
&\vdots \\
&a_{1n} y_1 + a_{2n} y_2 + \cdots + a_{mn} y_m \geq c_n \\
&y_i \geq 0, \quad i = 1, \ldots, m
\end{aligned}
\]
\end{multicols}
 \pause In matrix notation, we have (1) $\max\limits_X c^\top X$, subject to $AX \leq b$, $X \geq 0$ 
and (2) $\min\limits_Y Y^\top b$, subject to $A^\top Y \geq c$, $Y \geq 0$. (1) and (2) 
are called dual problems to each other. Note that every equality constraint can be translated into two 
inequality constraints (e.g.\ $w = c$ means exactly $w \geq c$ and $-w \geq -c$)

    }
\end{frame}
\begin{frame}{Linear Programming and Linear Pricing Measures}

    {\footnotesize \footnotesize
    Two key results in linear programming are:

\par Result 1 (Feasibility): For any LP problem, one of the following must hold:
\begin{enumerate}
\item[(a)] The LP has a finite optimal solution.
\item[(b)] The optimal solution is either $\infty$ or $-\infty$.
\item[(c)] The feasible region is empty, and thus the LP has no optimal solution.
\end{enumerate}
\vspace{1em}
% \par Result 2 
\vspace{1em}
\par  \pause Result 2 (Strong Duality):
If a LP problem has a finite optimal solution, then so does its dual,
 and the optimal values for both the primary and the dual problem must be equal.
 The proof idea uses the geometry of convex sets (via the Farkas lemma or separating hyperplane theorem)
 \vspace{1em}
\par \pause Another Result: (Weak Duality): For the primal-dual pair define above, if we take any feasible $x$
and any feasible $y$, we have the weak duaility $c^\top x\leq b^\top y$. Proof follows from the two inequalities:
$ c^\top x\leq y^\top Ax \leq y^\top b = b^\top y$.

    }
\end{frame}

\begin{frame}{Linear Programming and Linear Pricing Measures}

    {\footnotesize \footnotesize
    \par \textbf{Strong Duality Theorem (Linear Programming):}
    If the primal LP
    \[
    \max_{x} c^{\top} x \quad \text{s.t. } Ax \leq b, \, x \geq 0
    \]
    has a finite optimal solution $x^*$, then the dual LP
    \[
    \min_{y} b^{\top} y \quad \text{s.t. } A^{\top} y \geq c, \, y \geq 0
    \]
    also has an optimal solution $y^*$, and $ c^{\top} x^* = b^{\top} y^*.$
    \vspace{1em}
    \par  \pause This dual relationship is exactly how arbitrage-free pricing is framed:
    \begin{itemize}
        \item  The primal problem corresponds to ``finding a trading strategy to make profit'' (maximize wealth)
        \item  The dual problem corresponds to ``finding consistent state prices or probabilities'' (minimize cost)
        \item  no arbitrage $\Leftrightarrow$ both sides match.
    \end{itemize}
    }
\end{frame}

\begin{frame}{Linear Programming and Linear Pricing Measures}

    {\footnotesize \footnotesize
    \par \textbf{Theorem 1}: There are no dominant strategies 
    if and only if there exists a linear pricing (probability) measure.
    \vspace{1em}
    \par  \pause \textbf{Proof}:
    Let $S = (1, S_1^*(0), \ldots, S_N^*(0))^T$ and let
    \[
    Z =
    \begin{pmatrix}
    1 & \cdots & 1 \\
    S_1^*(1, \omega_1) & \cdots & S_1^*(1, \omega_K) \\
    \vdots & \ddots & \vdots \\
    S_N^*(1, \omega_1) & \cdots & S_N^*(1, \omega_K)
    \end{pmatrix}
    \]
    be all the possible outcomes of asset prices at time 1. Denote $h$ to be any trading strategy $h = (\phi_0, \phi_1, \ldots, \phi_N)$. Then the wealth process is given by
    \[
    V(0) = hS, \quad V^*(1) = hZ.
    \]
    \par \pause  Using part (b) of Exercise 4, we 
    see that the existence of a linear pricing measure implies no dominant strategy.


    }
\end{frame}

\begin{frame}{Linear Programming and Linear Pricing Measures}

    {\footnotesize \footnotesize
    \par Conversely, if there is no dominant strategy, 
    then the value of the linear programming problem $\min_h hS$, 
    subject to $hZ \geq 0$, has to be non-negative. But, since one can 
    always choose no trading at all, the value of the linear programming 
    problem has to be zero. Consequently, its dual problem must also have a value 
    zero. This, in conjunction with part (a) of Exercise 4, implies that 
    there exists a non-negative vector $\pi$, $Z\pi = S$,
     which is the linear pricing measure. The proof is terminated.
     \vspace{1em}
    \par  \pause \textbf{Theorem 2}: Absence of arbitrage opportunity is equivalent 
    to the existence of a risk-neutral measure.
    \vspace{1em}
    \par  \pause \textbf{Farkas' Lemma}
    \par Let $A$ be an $m \times n$ matrix and $b$ be an $m$-dimensional column vector. Then either

    \[
    Ax = b, \quad x \geq 0, \quad x = (x_1, \ldots, x_n)^\top
    \]

    has a solution, or

    \[
    yA \leq 0, \quad yb > 0, \quad y = (y_1, \ldots, y_m)
    \]

    has a solution, but not both.

    }
\end{frame}
\begin{frame}{Linear Programming and Linear Pricing Measures}

    {\footnotesize \footnotesize
     Now we start to prove that no arbitrage implies the existence of a risk neutral measure. Let $A$ be the $(K + 1) \times (2N + K)$ matrix:

    \[
    A = 
    \begin{pmatrix}
    0 & 0 & 0 & \cdots & 0 \\
    \Delta S^*_1(\omega_1) & -\Delta S^*_1(\omega_1) & \Delta S^*_2(\omega_1) & \cdots & -\Delta S^*_N(\omega_1) \\
    \Delta S^*_1(\omega_2) & -\Delta S^*_1(\omega_2) & \Delta S^*_2(\omega_2) & \cdots & -\Delta S^*_N(\omega_2) \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \Delta S^*_1(\omega_K) & -\Delta S^*_1(\omega_K) & \Delta S^*_2(\omega_K) & \cdots & -\Delta S^*_N(\omega_K)
    \end{pmatrix} \circ B,
    \]
     \pause where the notation $\circ$ means pasting two matrices together, and

    \[
    B = \begin{pmatrix}
    1 & 1 & \cdots & 1 \\
    -1 & 0 & \cdots & 0 \\
    0 & -1 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & -1
    \end{pmatrix}.
    \]

    }
\end{frame}

\begin{frame}{Linear Programming and Linear Pricing Measures}

    {\footnotesize \footnotesize
    
    \par \textbf{Lemma}: Denote $b = (1, 0, \ldots, 0)^T$ be a $(K + 1)$ column vector. Then

    \[
    Ax = b, \, x \geq 0, \, x = (x_1, \ldots, x_{2N+K})
    \]

    has a solution if and only if there exists an arbitrage opportunity.
    \vspace{1em}
    \par  \pause \textbf{Proof}:
    The equation $Ax = b$ leads to a system of equations:

    \begin{enumerate}
    \item[(a)] The first equation of the system says
    \[
    x_{2N+1} + \cdots + x_{2N+K} = 1,
    \]
    which implies that $x_{2N+k} > 0$ for at least one value of $k$.

    \item[(b)] For $i \geq 2$ the $i$th equation is
    \[
    \Delta S_1^*(\omega_{i-1})(x_1 - x_2) + \cdots + \Delta S_N^*(\omega_{i-1})(x_{2N-1} - x_{2N}) = x_{2N+i-1}.
    \]
    \end{enumerate}
    \par \pause  ($\Rightarrow$) Suppose $(x_1, x_2, \ldots, x_{2N+K})^T$ is a solution. Let a trading strategy be $\phi_n = x_{2n-1} - x_{2n}$ for $n = 1, 2, \ldots, N$. Then, by the observation (b) above, we have the discounted gain is
    \[
    G^* = \Delta S_1^*(\omega_k)\phi_1 + \cdots + \Delta S_N^*(\omega_k)\phi_N = x_{2N+k}, \quad k = 1, \ldots, K.
    \]
    }
\end{frame}
\begin{frame}{Linear Programming and Linear Pricing Measures}

    {\footnotesize \footnotesize
    Since $x_{2N+k} \geq 0$ for $k = 1, \ldots, K$, this system of equations says $G^* \geq 0$. Since $x_{2N+k} > 0$ for at least one value of $k$, by observation (a), the left hand side of the equation displayed above must be strictly positive for at least one $k$. Hence $\mathbb{E}[G^*] > 0$, and $(\phi_1, \ldots, \phi_N)$ is an arbitrage strategy.
    \vspace{1em}
    \par   \pause ($\Leftarrow$) Conversely, suppose there exists an arbitrage opportunity $\phi$, in which case $\mathbb{E}[G^*] > 0$, and
\[
G^* = \sum_{n=1}^N \phi_n \Delta S_n^* \geq 0.
\]

There's at least one state $\omega_i$ such that $G^*(\omega_i) > 0$. Therefore, we can find a positive constant $\lambda$ such that
\[
\lambda \sum_{k=1}^K G^*(\omega_k) = \lambda \sum_{k=1}^K \sum_{n=1}^N \phi_n \Delta S_n^*(\omega_k) = 1.
\]
This can be done by taking $\lambda = 1 / \sum_{k=1}^{K} G^*(\omega_k)$.
    }
\end{frame}


\begin{frame}{Linear Programming and Linear Pricing Measures}

    {\footnotesize \footnotesize
    \begin{enumerate}
\item[(i)] If $\phi_n = 0$, set $x_{2n-1} = x_{2n} = 0$, in which case $x_{2n-1} - x_{2n} = 0 = \lambda \phi_n$.
\item[(ii)] If $\phi_n > 0$, set $x_{2n-1} = \lambda \phi_n$ and $x_{2n} = 0$, in which case $x_{2n-1} - x_{2n} = \lambda \phi_n$.
\item[(iii)] If $\phi_n < 0$, set $x_{2n-1} = 0$, $x_{2n} = -\lambda \phi_n$, in which case $x_{2n-1} - x_{2n} = \lambda \phi_n$.
\item[(iv)] For $k = 1, 2, \ldots, K$, set
\[
x_{2N+k} = \Delta S_1^*(\omega_k)(x_1 - x_2) + \cdots + \Delta S_N^*(\omega_k)(x_{2N-1} - x_{2N}).
\]
\end{enumerate}
 \pause Then we have all $x_i \geq 0, i = 1, 2, \ldots, 2N$, and
\begin{gather*}
    x_{2N+k} = \Delta S_1^*(\omega_k)\lambda \phi_1 + \cdots + \Delta S_N^*(\omega_k)\lambda \phi_N 
    = \lambda G^*(\omega_k) \geq 0, \quad k = 1, \ldots, K \\
    x_{2N+1} + \cdots + x_{2N+K} = \sum_{k=1}^{K} \sum_{n=1}^{N} \lambda \phi_n \Delta S_n^*(\omega_k) = 1,
\end{gather*}


Therefore, $(x_1, \ldots, x_{2N+K})$ is a solution to the problem, and the proof of the lemma is terminated.

    }
\end{frame}

\begin{frame}{Linear Programming and Linear Pricing Measures}

    {\footnotesize \footnotesize
     By the above lemma and Farkas's lemma, the absence of arbitrage implies that there is a solution to the equation
\[
yA \leq 0, \quad yb > 0, \quad y = (y_0, y_1, \ldots, y_K).
\]

Since $b = (1, 0, \ldots, 0)^T$, we have $y_0 = yb > 0,$ and $yA \leq 0$ implies three sets of inequalities:
\begin{gather*}
    \sum_{j=1}^{K} \Delta S_n^*(\omega_j) y_j \leq 0, \quad \sum_{j=1}^{K} - \Delta S_n^*(\omega_j) y_j \leq 0, \quad n = 1, \ldots, N, \\
    y_0 - y_n \leq 0, \quad n = 1, \ldots, K.
\end{gather*}


 \pause Therefore, the first two sets of inequalities above must be equalities, i.e.
\begin{align*}
\sum_{j=1}^{K} \Delta S_n^*(\omega_j) y_j &= 0, \quad n = 1, \ldots, N; \quad 
0 < y_0 \leq y_n, \quad n = 1, \ldots, K.
\end{align*}

In summary, under these weights, each asset's discounted price is a martingale. 
The weights $y_j > 0$ can be normalized to sum to one, 
giving a probability measure $Q(\omega_j) = y_j / \sum_i y_i$. Thus, $Q$ is a risk-neutral probability measure.
% $(y_1, \ldots, y_K)$ forms a risk-neutral probability measure, and the theorem is proved.


    }
\end{frame}

% \begin{frame}

%     {\footnotesize \footnotesize

%     }
    
% \end{frame}
% % {\mathbb{P}^*}
% \tilde{\mathbb{P}}
% {\footnotesize \footnotesize
% }
% \tiny
% \scriptsize
% \footnotesize
% \small
% \normalsize (default)
\end{document}