\documentclass{beamer}

% \usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}
\usepackage{lmodern}   % modern Latin Modern fonts
\usepackage{textcomp}  % provides \textquoteright
\usepackage{lmodern} % Latin Modern fonts with T1 shapes


\usepackage{graphicx}
\usepackage{ragged2e} % for generating dummy text
\usepackage[backend=biber,style=authoryear]{biblatex}
% \addbibresource{references.bib}

\usetheme{Madrid}
\usecolortheme{default}
\usefonttheme{professionalfonts} % keeps proper math fonts

\usepackage{amsmath,amssymb,amsfonts} % math symbols (\mathcal, \mathbb, etc.)
\usepackage{mathrsfs}    
\usepackage{multicol}             % optional: \mathscr for fancy script

% \setbeamercovered{invisible} 
\setbeamercovered{transparent}


\title{MF921 Topics in Dynamic Asset Pricing}
\subtitle{Week 10}
\author{Yuanhui Zhao}
\date{Boston University}

\begin{document}
\frame{\titlepage}
% \begin{frame}
% \frametitle{Outline}
% \tableofcontents
% \end{frame}


\begin{frame}{Chapter 14}

    {
    \begin{center}
        Chapter 14 Viscosity Solutions and HJB Equations
    \end{center}
    }
    
\end{frame}


% \begin{frame}{Definition of Viscosity Solutions}

%     {\footnotesize \footnotesize
%      We start with an open domain  
% \[
% \Omega \subset \mathbb{R}^d,
% \]
% and a function \( u(t, x) \) satisfying a nonlinear second-order PDE  
% \[
% F(t, x, u(t, x), D_t u(t, x), D_x u(t, x), D_x^2 u(t, x)) = 0, \quad (t, x) \in [0, T) \times \Omega.
% \]
% Where :  
%     \begin{itemize}
%         \item \( D_t u \): time derivative \(\partial u / \partial t\)  
%         \item \( D_x u = \nabla_x u = \left( \frac{\partial u}{\partial x_1}, \ldots, \frac{\partial u}{\partial x_d} \right)^T \)  
%         \item \( D_x^2 u \): the Hessian matrix, with entries \((D_x^2 u)_{ij} = \frac{\partial^2 u}{\partial x_i \partial x_j}\)
%     \end{itemize}
%      with the terminal condition
%     \[
% u(T, x) = g(x).
% \]
% This is typical for backward PDEs (as in HJB equations).  
% For infinite-horizon problems, there's no finite terminal time \( T \), so this condition disappears.
%     }
   
    
% \end{frame}
% \begin{frame}{Definition of Viscosity Solutions}

%     {\footnotesize \footnotesize
%     Before defining viscosity solutions, we require \( F \) to behave "nicely" under 
%     perturbations, this ensures the notion of viscosity sub/supersolutions makes sense.
%     \begin{itemize}
%         \item (i) Ellipticity condition: for symmetric matrices \( M, \hat{M} \):

% \[
% M \leq \hat{M} \Rightarrow F(t, x, u, q, p, M) \geq F(t, x, u, q, p, \hat{M}), \quad (t, x) \in [0, T) \times \Omega.
% \]
% So ellipticity ensures \( F \) is nonincreasing in the second derivative argument.
%         \item (ii) Parabolicity condition: for the time derivative variable \( q \):  

% \[
% q \leq \hat{q} \Rightarrow F(t, x, u, q, p, M) \geq F(t, x, u, \hat{q}, p, M), \quad (t, x) \in [0, T) \times \Omega.
% \]

%     \end{itemize}
%     A main motivation for viscosity: many HJB equations (or other nonlinear PDEs) 
%     have nonsmooth solutions — the value function \( v(t, x) \) is 
%     typically not differentiable.
% So we can't plug \( v \) into the PDE in the classical sense (because \( Dv \) and \( D^2v \) don't exist everywhere).

% Viscosity theory solves this by testing the PDE against smooth functions that touch \( v \) locally.
%     }
   
    
% \end{frame}

% \begin{frame}{Definition of Viscosity Solutions}

%     {\footnotesize \footnotesize
%      Definition. Assume both the ellipticity and parabolicity conditions are satisfied.
%      \begin{itemize}
%         \item A continuous function \(u:\Omega\to\mathbb{R}\) is a viscosity subsolution of the above PDE
% if for any \(C^{1}\times C^{2}\)  function \(\phi\) that touches \(u\) from above 
% and any local maximum point \((t,y)\in[0,T)\times\Omega\) of \(u-\phi\) we have
%     \[
%     F(t,y,u(t,y),D_{t}\phi(t,y),D_{x}\phi(t,y),D_{x}^{2}\phi(t,y))\leq 0,
%     \]
%     and
%     \[
%     u(T,x)\leq g(x).
%     \]  
%         \item A continuous function \(u:\Omega\to\mathbb{R}\) is a viscosity 
%         supersolution of the above PDE if for any \(C^{1}\times C^{2}\) 
%         function \(\phi:[0,T)\times\Omega\to\mathbb{R}\) and any local minimum point \((t,y)\in[0,T)\times\Omega\) of \(u-\phi\) we have
%     \[
%     F(t,y,u(t,y),D_{t}\phi(t,y),D_{x}\phi(t,y),D_{x}^{2}\phi(t,y))\geq 0,
%     \]
%     and
%     \[
%     u(T,x)\geq g(x).
%     \]
%     \(\phi\) is called a test function. 
%     If \(u\) is both a viscosity subsolution and a viscosity supersolution, then \(u\) is called a 
%     viscosity solution (necessarily with \(u(T,x)=g(x)\)).
%      \end{itemize}
%     }
   
    
% \end{frame}

% \begin{frame}{Definition of Viscosity Solutions}

%     {\footnotesize \footnotesize
%       Lemma 1
%       \begin{itemize}
%         \item[(i)] A classical solution is a viscosity solution.
%         \item[(ii)] A \(C^{1}\times C^{2}\) viscosity solution is a classical solution.
%       \end{itemize}
%     \par \textbf{Proof}
%     \par $(i)$ Suppose $u$ is a classical solution, i.e., $C^{1}\times C^{2}$ and satisfying the
% PDE. For any test function $\phi$ and any local maximum point
% $(t,y)\in[0,T)\times \Omega$ of $u-\phi$ we have
% \[
% D_x u(t,y)=D_x \phi(t,y),\qquad D_x^2 u(t,y)\le D_x^2 \phi(t,y),
% \]
% because $\Omega\subset\mathbb{R}^d$ is an open domain, and the first–order inequality holds,
% \[
% D_t u(t,y)\le D_t \phi(t,y),
% \]
% because the maximum point may be at the boundary $t=0$. Thus,
% \begin{align*}
% &F\bigl(t,y,u(t,y), D_t\phi(t,y), D_x\phi(t,y), D_x^2\phi(t,y)\bigr)\\
% &\qquad= F\bigl(t,y,u(t,y), D_t\phi(t,y), D_x u(t,y), D_x^2\phi(t,y)\bigr)\\
% &\qquad\le F\bigl(t,y,u(t,y), D_t\phi(t,y), D_x u(t,y), D_x^2 u(t,y)\bigr)\\
% &\qquad\le F\bigl(t,y,u(t,y), D_t u(t,y), D_x u(t,y), D_x^2 u(t,y)\bigr)\\
% &\qquad= 0,
% \end{align*}
% where the first and second inequalities follow from the ellipticity and parabolicity
% conditions, respectively. Hence $u$ is a viscosity subsolution. Similarly, $u$ is a
% viscosity supersolution. Therefore, $u$ is a viscosity solution.
%     }
   
    
% \end{frame}

% \begin{frame}{Definition of Viscosity Solutions}

%     {\footnotesize \footnotesize
%     (ii) Suppose $u$ is a viscosity solution and is $C^{1}\times C^{2}$. Then we can take
% $\phi=u$. We have any point $(t,y)\in[0,T)\times \Omega$ is both a local maximum point and local
% minimum point of $u-\phi$. Thus,
% \begin{align*}
% &F\bigl(t,y,u(t,y), D_t u(t,y), D_x u(t,y), D_x^2 u(t,y)\bigr)\\
% &\qquad= F\bigl(t,y,u(t,y), D_t\phi(t,y), D_x\phi(t,y), D_x^2\phi(t,y)\bigr)\\
% &\qquad= 0,
% \end{align*}
% where the last equality comes from the definitions of subsolution and supersolution.
% This shows that $u$ is a classical solution.
% \vspace{1em}
% \par \textbf{Remark:}
% \par $1)$: For the infinite-horizon problem, the first term \( t \) and \( D_t \) is dropped from \( F \), i.e., we have  
%     \[
%     F(x, u(x), D_x u(x), D_x^2 u(x)) = 0, \quad x \in \Omega,
%     \]
%     the terminal condition disappears, and we 
%     do not need the parabolicity condition. For a finite-horizon deterministic control problem, the term \( D_x^2 \) is dropped from \( F \), i.e., we have  
%     }
   
    
% \end{frame}
% \begin{frame}{Definition of Viscosity Solutions}

%     {\footnotesize \footnotesize
%     \[
%     F(t, x, u(t, x), D_t u(t, x), D_x u(t, x)) = 0, \quad (t, x) \in [0, T) \times \Omega,
%     \]
%     and we do not need the ellipticity condition. For an infinite-horizon deterministic control problem, we have  
%     \[
%     F(x, u(x), D_x u(x)) = 0, \quad x \in \Omega,
%     \]
%     for which both ellipticity and parabolicity conditions are not needed.
%     \vspace{1em}
%     \par $2)$: For any viscosity subsolution, we can always choose the new test function \(\hat{\phi}\) touches \(u\) at one point, which is the local maximum point of \(u - \hat{\phi}\), and \(\hat{\phi}\) is above the subsolution \(u\). Indeed, for any viscosity subsolution and a test function at any local maximum point \((t_0, y_0) \in [0, T) \times \Omega\) of \(u - \phi\), we have
%     \[
%     u(t, x) - \phi(t, x) \leq u(t_0, y_0) - \phi(t_0, y_0), \quad \forall (t, x) \in N_{(t_0, y_0)},
%     \]
%     where \(N_{(t_0, y_0)}\) is a sufficiently small neighborhood of \((t_0, y_0)\) within \([0, T) \times \Omega\). We can define a new test function
%     \[
%     \hat{\phi}(t, x) = \phi(t, x) + u(t_0, y_0) - \phi(t_0, y_0).
%     \]
%     Then
%     \[
%     u(t_0, y_0) = \hat{\phi}(t_0, y_0),
%     \]
%     \[
%     u(t, x) - \hat{\phi}(t, x) = u(t, x) - \phi(t, x) - u(t_0, y_0) + \phi(t_0, y_0) \leq 0, \quad \forall (t, x) \in N_{(t_0, y_0)}.
%     \]
%     }
   
    
% \end{frame}

% \begin{frame}{Definition of Viscosity Solutions}

%     {\footnotesize \footnotesize
%       Thus, the new test function \(\hat{\phi}\) touches \(u\) at one 
%       point \((t_0, y_0)\), which is the local maximum point of \(u - \hat{\phi}\), 
%       and \(\hat{\phi}\) is above the subsolution \(u\). Similarly, for any supersolution \(u\), 
%       there is a test function \(\hat{\phi}\) that touches \(u\) at a local 
%       minimum point of \(u - \hat{\phi}\), and \(\hat{\phi}\) is below the supersolution \(u\).
%       \vspace{1em}
%     \par $3)$: The fact that \( u \) is a viscosity solution to the PDE \( F = 0 \) does
%     not imply that \( u \) is a viscosity solution to the PDE \(-F = 0\).
%     }
   
    
% \end{frame}
% \begin{frame}{Connection with the HJB Equation: An Overview}

%     {\footnotesize \footnotesize
%        For a finite-horizon stochastic control for diffusion process,

% \[
% v(t, x) = \sup_{u_t \in U} E \left[ \int_0^T e^{-\beta s} c(X_s, u_s) ds 
% + e^{-\beta T} g(X_T) \middle| X_t = x \right], 
% \]

% \[
% dX_t = b(X_t, u_t) dt + \sigma(X_t, u_t) dW_t, \quad t \geq 0,
% \]

% where \( b : \mathbb{R}^d \times U \rightarrow \mathbb{R}^d \) is the drift and \(\sigma(X_t, u_t)\) is the volatility matrix. The HJB equation is given by

% \[
% -\frac{\partial v(t, x)}{\partial t} + \beta v(t, x) - 
% \sup_{u \in U} [A_u v(t, x) + c(x, u)] = 0, 
% \]
% \[
% v(T, x) = g(x),
% \]
% where

% \[
% A_u v(t, x) = \sum_{i=1}^d b_i (x, u) \frac{\partial v}{\partial x_i} + \frac{1}{2} \sum_{i=1}^d \sum_{j=1}^d a_{ij}(x, u) \frac{\partial^2 v}{\partial x_i \partial x_j}
% \]
% \[
% = b^\top (x, u) \cdot Dv + \frac{1}{2} \text{Su} \left( a(x, u) \circ D^2 v \right),
% \]


%     }
   
    
% \end{frame}
% \begin{frame}{Connection with the HJB Equation: An Overview}

%     {\footnotesize \footnotesize
%     and the matrix \( a(x, u) = \sigma(x, u) \sigma^\top (x, u) \) is assumed to be positive semi-definite, 
%     \(\circ\) denotes the Hadamard product, and \( \text{Su}(A) \) denotes
%      the sum of all the elements in the matrix \( A \). In this case the
%       finite-horizon HJB equation can be written as

% \[
% F(t, x, r, q, p, M) = -q + \beta r - \sup_{u \in U} \left[ b^\top (x, u) 
% \cdot p + \frac{1}{2} \text{Su} \left( a(x, u) \circ M \right) + c(x, u) \right].
% \]

% \par The parabolicity condition is automatically satisfied thanks to the \(-q\) 
% term above. Next, we shall prove that the ellipticity condition is satisfied.
% \vspace{1em}
% \par By the Schur product theorem, the Hadamard product of two positive semidefinite matrices is positive semidefinite. Thus, \(a(x,u) \circ (\hat{M} - M)\) is positive semidefinite, if \(\hat{M} - M\) is positive semidefinite. Therefore,

% \[
% \text{Su}(a(x,u) \circ (\hat{M} - M)) = 1^T \left( a(x,u) \circ (\hat{M} - M) \right) 1 \geq 0,
% \]

% where \(1^T = (1, ..., 1)\) is a \(d\)-dimensional row vector, yielding
% \[
% b^T(x,u) \cdot p + \frac{1}{2} \text{Su} \left( a(x,u) \circ M \right) + c(x,u) \leq b^T(x,u) \cdot p + \frac{1}{2} \text{Su} \left( a(x,u) \circ \hat{M} \right) + c(x,u).
% \]

%     }
   
    
% \end{frame}
% \begin{frame}{Connection with the HJB Equation: An Overview}

%     {\footnotesize \footnotesize
%      Taking sup gives

% \[
% b^T(x,u) \cdot p + \frac{1}{2} \text{Su} \left( a(x,u) \circ M \right) 
% + c(x,u) \leq \sup_{u \in U} \left[ b^T(x,u) \cdot p + \frac{1}{2} \text{Su} 
% \left( a(x,u) \circ \hat{M} \right) + c(x,u) \right].
% \]

% Repeating the sup leads to
% {\footnotesize \tiny
% \[
% \sup_{u \in U} \left[ b^T(x,u) \cdot p + \frac{1}{2} \text{Su} \left( a(x,u) 
% \circ M \right) + c(x,u) \right] \leq \sup_{u \in U} \left[ b^T(x,u) \cdot p 
% + \frac{1}{2} \text{Su} \left( a(x,u) \circ \hat{M} \right) + c(x,u) \right].
% \]
% }
% Hence

% \[
% F(t,x,r,q,p,M) \geq F(t,x,r,q,p,\hat{M}),
% \]
% \par and the ellipticity condition holds, so that we can define the 
% viscosity solution for the finite-horizon HJB equation.
% \par he viscosity solution is connected to the HJB equation. With suitable
%  regularity conditions, it can be shown that the value function of stochastic control problem  is
%  the unique bounded viscosity solution of the finite-horizon HJB equation.
%     }
   
    
% \end{frame}
% \begin{frame}{Infinite-Horizon
% Deterministic Control and the First-Order
% HJB equation}

%     {\footnotesize \footnotesize
%       Consider the following deterministic control problem
% \[
% v(x) = \sup_{u_t \in U} \int_0^\infty e^{-\beta t} c(X_t, u_t) dt,
% \]
% with state dynamics
% \[
% dX_t = f(X_t, u_t)dt, \quad t \geq 0, \quad X(0) = x.
% \]

% The HJB equation is given by
% \[
% \beta v(x) - \sup_{u \in U} [Dv(x) \cdot f(x, u) + c(x, u)] = 0. 
% \]
% \par Regularity Conditions:
% \begin{itemize}
%     \item The set of controls \(U\) is compact.
%     \item The state dynamic function \(f : \mathbb{R}^d \times U \rightarrow \mathbb{R}^d\) is continuous. Furthermore, \(f\) is Lipschitz continuous (a strong form of uniform continuous) in the first variable, i.e., there exists a constant \(L > 0\) such that
%     \[
%     \|f(x, u) - f(y, u)\| \leq L \|x - y\|, \quad x, y \in \mathbb{R}^d, \quad u \in U.
%     \]
% \end{itemize}
%     }
   
    
% \end{frame}

% \begin{frame}{Infinite-Horizon
% Deterministic Control and the First-Order
% HJB equation}

%     {\footnotesize \footnotesize
%     \begin{itemize}
%         \item The cost function \(c : \mathbb{R}^d \times U \rightarrow \mathbb{R}\) is continuous and uniformly bounded. Furthermore, \(c\) is Lipschitz continuous in the first variable, there exists a constant \(L > 0\) such that
%     \[
%     |c(x, u) - c(y, u)| \leq L \|x - y\|, \quad x, y \in \mathbb{R}^d, \quad u \in U.
%     \]
%     \end{itemize}
%     \par We assume the above regularity conditions hold for all the results below.

% Note that the HJB equation is a special case of the nonlinear first order PDE
% \[
% F(x, u(x), Du(x)) = 0, \quad x \in \Omega. \tag{14.6}
% \]

% \par Since the elliptical and parabolic conditions are automatically satisfied for the first order PDE (14.6), we can define viscosity solutions to (14.6).
% \vspace{1em}
% \par Theorem 1. The value function \(v\) is the unique uniformly 
% bounded viscosity solution to the HJB equation.

% Note that unbounded viscosity solutions may exist, and they are not related to the value function of the control problem.

%     }
   
    
% \end{frame}

% \begin{frame}{Continuity and the Dynamic Programming Principle}

%     {\footnotesize \footnotesize
%      First recall the Gronwall inequality: Let \( A(t) \), \(\Psi(t)\) and \(\chi(t)\) be real continuous functions defined on \( t \in [a, b] \) with \( A(t) \geq 0 \). If
% \[
% A(t) \leq \Psi(t) + \int_a^t A(s)\chi(s)ds, \quad t \in [a, b],
% \]
% then
% \[
% A(t) \leq \Psi(t) + \int_a^t \chi(s)\Psi(s) \exp\left[\int_s^t \chi(u)du\right] ds, \quad t \in [a, b].
% \]
% \vspace{1em}
% \par Lemma 2: Suppose \( X_t^x \) and \( X_t^y \) are the state processes with the same control policy \( u_t \) but different starting values \( x \) and \( y \), i.e., \( X_t^x(u_t) \) has the dynamic  
% \[
% dX_t = f(X_t, u_t)dt, \quad X_0 = x.
% \]  
% Then we have  
% \[
% \|X_t^x - X_t^y\| \leq e^{Lt} \|x - y\|, \quad 0 \leq t \leq T.
% \]
     
%     }
   
    
% \end{frame}
% \begin{frame}{Continuity and the Dynamic Programming Principle}

%     {\footnotesize \footnotesize
%     \par [Proof]
%     \par We have  
% \[
% X_t^x = x + \int_0^t f(X_s^x, u_s)ds, \quad X_t^y = y + \int_0^t f(X_s^y, u_s)ds.
% \]  
% Thus,  
% \[
% X_t^x - X_t^y = x - y + \int_0^t f(X_s^x, u_s) - f(X_s^y, u_s)ds,
% \]  
% from which we get  
% \[
% \|X_t^x - X_t^y\| \leq \|x - y\| + \int_0^t \|f(X_s^x, u_s) - f(X_s^y, u_s)\| ds
% \]  
% \[
% \leq \|x - y\| + L \int_0^t \|X_s^x - X_s^y\| ds.
% \]  

%     }
% \end{frame}

% \begin{frame}{Continuity and the Dynamic Programming Principle}

%     {\footnotesize \footnotesize
    
%      Then the Gronwall inequality gives  
% \[
% \|X_t^x - X_t^y\| \leq \|x - y\| + \int_0^t L \|x - y\| e^{L(t-s)} ds
% \]  
% \[
% = \|x - y\| + L \|x - y\| e^{Lt} \left( \frac{1}{L} (1 - e^{-Lt}) \right)
% \]  
% \[
% = \|x - y\| + \|x - y\| \left( e^{Lt} - 1 \right)
% \]  
% \[
% = \|x - y\| e^{Lt},
% \]  
% from which the conclusion follows.
% \vspace{1em}
% \par Proposition 1 : The value function \( v \) is uniformly bounded and uniformly continuous.
% \vspace{1em}
% \par [Proof]
% \par The uniformly boundedness follows from the fact that \( |c(x, u)| \leq C \) and \(\int_0^\infty e^{-\beta t}Cdt < \infty\).

% Fix an arbitrary \(\varepsilon > 0\). Consider \( x, y \in \mathbb{R}^d\). There exists a control policy \( u_t^\varepsilon \) such that
% \[
% v(y) \leq \int_0^\infty e^{-\beta t}c(X_t^y, u_t^\varepsilon)dt + \varepsilon,
% \]
%     }
   
    
% \end{frame}

% \begin{frame}{Continuity and the Dynamic Programming Principle}

%     {\footnotesize \footnotesize
%      \( X_t^y \) means that \( X_t^y \) starts from \( y \) (i.e., \( X_0^y = y \)), thanks to the definition of sup. Since \( u_t^\varepsilon \) is one of the control policies, we also have
% \[
% v(x) \geq \int_0^\infty e^{-\beta t}c(X_t^x, u_t^\varepsilon)dt.
% \]

% Thus,
% \[
% v(y) - v(x) \leq \int_{0}^{\infty} e^{-\beta t} [c(X_t^y, u_t^{\varepsilon}) - c(X_t^x, u_t^{\varepsilon})] \, dt + \varepsilon.
% \]

% Since \( c \) is uniformly bounded, we can find \( T > 0 \), independent of \( x \) and \( y \), such that
% \[
% \int_{T}^{\infty} e^{-\beta t} [c(X_t^y, u_t^{\varepsilon}) - c(X_t^x, u_t^{\varepsilon})] \, dt < \varepsilon.
% \]
% Therefore,
% \[
% v(y) - v(x) \leq \int_{0}^{T} e^{-\beta t} [c(X_t^y, u_t^{\varepsilon}) - c(X_t^x, u_t^{\varepsilon})] \, dt + 2\varepsilon
% \]
% \[
% \leq L \int_{0}^{T} e^{-\beta t} \|X_t^y - X_t^x\| \, dt + 2\varepsilon,
% \]
% via the Lipschitz condition.
 
%     }
   
    
% \end{frame}
% \begin{frame}{Continuity and the Dynamic Programming Principle}

%     {\footnotesize \footnotesize
% Switching \( x \) and \( y \) in the above argument leads to
% \[
% v(x) - v(y) \leq L \int_{0}^{T} e^{-\beta t} \|X_t^y - X_t^x\| \, dt + 2\varepsilon.
% \]
% In summary,
% \[
% |v(x) - v(y)| \leq L \int_{0}^{T} e^{-\beta t} \|X_t^y - X_t^x\| \, dt + 2\varepsilon.
% \]

% Using Lemma 2, we have
% \[
% |v(x) - v(y)| \leq L \|y - x\| \int_{0}^{T} e^{(L - \beta)t} \, dt + 2\varepsilon.
% \]

% Thus, there exists a \( \delta \) (independent of \( x \) and \( y \)) such that
% \[
% |v(x) - v(y)| \leq 3\varepsilon,
% \]
% as long as \( \|y - x\| \leq \delta \). Thus, \( v \) is uniformly continuous.

%     }
   
    
% \end{frame}
% \begin{frame}{Continuity and the Dynamic Programming Principle}

%     {\footnotesize \footnotesize
%  Proposition 2:  (Dynamic Programming Principle) For every \( x \in \mathbb{R}^d \) and \( t \geq 0 \),

% \[
% v(x) = \sup_{u \in \mathcal{U}} \left[ \int_0^t e^{-\beta s} c(X_s, u_s) ds + e^{-\beta t} v(X_t) \right].
% \]

%     }
   
    
% \end{frame}

\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
    Lemma 3: For every \( x \in \mathbb{R}^d \) and every open neighborhood of \( x \), denoted by \( N_x \), there exists \( t_x > 0 \) small enough, such that for any control policy \( u \), the state dynamics given by
\[
dX_t = f(X_t, u_t)dt, \quad X_0 = x,
\]
satisfies
\[
X_t \in N_x, \quad \forall 0 \leq t \leq t_x.
\]
    \par Intuitively, $f(x,u)$ is bounded and Lipschitz continuous, your system can’t move too far away from its starting point 
$x$ in an infinitesimally short time.
\vspace{1em}
 \pause 
\par [Proof]
\par Since \( f \) is continuous and \( U \) is compact, we have for every \( x \in \mathbb{R}^d \),
\[
M_x = \sup_{u \in U} \|f(x, u)\| < \infty.
\]

Because
\[
X^x_t = x + \int_0^t f(X^x_s, u_s)ds = x + \int_0^t f(x, u_s)ds + \int_0^t [f(X^x_s, u_s) - f(x, u_s)] ds,
\]
we have, via the Lipschitz continuity,
\[
\|X^x_t - x\| \leq M_x t + L \int_0^t \|X^x_s - x\| ds.
\]

    }
   
    
\end{frame}

\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
    The Gronwall inequality yields that
    \begin{align*}
        \|X^x_t - x\| &\leq M_x t + \int_0^t LM_x s \exp [L(t - s)] ds\\
        &\leq M_x t + LM_x t \int_0^t \exp [L(t - s)] ds\\
         &\leq M_x t + M_x t(e^{Lt}-1)\\
        &= M_x te^{Lt},
    \end{align*}
\par from which the conclusion follows. 
\vspace{1em}
\par  \pause Proposition 3: The value function \( v \) is a viscosity supersolution to the HJB equation
\[
\beta v(x) - \sup_{u \in U} [Dv(x) \cdot f(x,u) + c(x, u)] = 0.
\]
\vspace{1em}
 \pause 
\par [Proof]
\par Consider a \( C^1 \) test function \( \phi \), and let \( y \) be local minimum of \( v - \phi \). There exists a neighborhood of \( y \), say \( N_y \), such that
\[
v(x) - \phi(x) \geq v(y) - \phi(y), \quad x \in N_y.
\]
 

    }
   
    
\end{frame}
\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
    
Thus,
\[
v(x) - v(y) \geq \phi(x) - \phi(y), \quad x \in N_y.
\]
By Lemma 3, for any control policy \( u_t \in U \) there exists \( t_y \) small enough such that  
\[
X_0 = y, \quad X_t \in N_y, \quad \forall 0 \leq t \leq t_y.
\]  
Thus,  
\[
v(X^y_t) - v(y) \geq \phi(X^y_t) - \phi(y), \quad \forall 0 \leq t \leq t_y.
\]  
 \pause For any control policy \( u_t \in U \) and \( u_0 = u \), the dynamic programming principle from Proposition 2 yields  
\[
v(y) \geq \int_0^t e^{-\beta s} c(X^y_s, u_s) ds + e^{-\beta t} v(X^y_t).
\]  
Therefore, adding the two inequalities leads to, \( 0 \leq t \leq t_y, \)  
\[
v(X^y_t) \geq \int_0^t e^{-\beta s} c(X^y_s, u_s) ds + e^{-\beta t} v(X^y_t) + \phi(X^y_t) - \phi(y).
\]  
    }
   
    
\end{frame}
\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
    Hence for \( 0 \leq t \leq t_y, \)  
\[
v(X^y_t) \left( \frac{1 - e^{-\beta t}}{t} \right) - \frac{1}{t} 
\int_0^t e^{-\beta s} c(X^y_s, u_s) ds \geq \frac{\phi(X^y_t) - \phi(y)}{t}. \tag{14.7}
\]  
Letting \( t \to 0 \),
 \pause 
\begin{itemize}
    \item  From Proposition 1, we know \( v \) is uniformly continuous.
    As \( t \to 0 \), the trajectory \( X_t^y \) starting at \( y \) satisfies \( X_t^y \to y \) (because \( f \) is continuous).
    Hence
    \[
    v(X_t^y) \longrightarrow v(y).
    \]
     \item  \pause \[ \frac{1 - e^{-\beta t}}{t} \to \beta \]
    
    This is the Taylor expansion of \( e^{-\beta t} \):
    \[
    e^{-\beta t} = 1 - \beta t + o(t) \Longrightarrow \frac{1 - e^{-\beta t}}{t} \to \beta.
    \]
    
\end{itemize}
    }
   
    
\end{frame}

\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
  
\begin{itemize}
     \item \[ \frac{1}{t} \int_0^t e^{-\beta s} c(X_s^y, u_s) \, ds \to c(y, u_0) \]
   
    For small \( t \), the control \( u_s \) is nearly constant at its initial value \( u_0 \),
    and \( X_s^y \approx y + f(y, u_0) s \), so \( X_s^y \to y \). By continuity of \( c \),
    \[
    e^{-\beta s} c(X_s^y, u_s) \to c(y, u_0).
    \]
    
    Then by the mean value theorem for integrals, the average converges.
     \item \pause  \[ \frac{\phi(X_t^y) - \phi(y)}{t} \to D\phi(y) \cdot f(y, u_0) \]
    
    By first-order Taylor expansion:
    \[
    \phi(X_t^y) = \phi(y) + D\phi(y) \cdot (X_t^y - y) + o(\|X_t^y - y\|).
    \]
    
     \pause But
    \[
    \frac{X_t^y - y}{t} = \frac{1}{t} \int_0^t f(X_s^y, u_s) \, ds \to f(y, u_0).
    \]
    
    So indeed:
    \[
    \frac{\phi(X_t^y) - \phi(y)}{t} \to D\phi(y) \cdot f(y, u_0).
    \]
\end{itemize}
    }
   
    
\end{frame}

\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
   Take limits on both sides of (14.7) as \( t \to 0 \):

Left-hand side:
\[
v(X_t^y) \left( \frac{1 - e^{-\beta t}}{t} \right) - \frac{1}{t} \int_0^t e^{-\beta s} c(X_s^y, u_s) \, ds \longrightarrow \beta v(y) - c(y, u_0).
\]

Right-hand side:
\[
\frac{\phi(X_t^y) - \phi(y)}{t} \longrightarrow D\phi(y) \cdot f(y, u_0).
\]

 \pause Now combine them and rearrange:
\[
\beta v(y) \geq D\phi(y) \cdot f(y, u_0) + c(y, u_0).
\]
Since \( u_0 \) is arbitrary, we get
\[
\beta v(y) \geq \max_{u \in U} \{D\phi(y) \cdot f(y, u) + c(y, u)\}.
\]

 \pause In other words
\[
\beta v(y) - \sup_{u \in U} \{D\phi(y) \cdot f(y, u) + c(y, u)\} \geq 0,
\]
which shows that \( v \) is a supersolution of the HJB equation.

    }
   
    
\end{frame}

\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
    Proposition 4: The value function \( v \) is a viscosity subsolution to the HJB equation
    \[
    \beta v(x) - \sup_{u \in U} [Dv(x) \cdot f(x,u) + c(x, u)] = 0.
    \]
    \vspace{1em}
     \pause 
    \par [Proof]
    \par Consider a \( C^1 \) test function \( \phi \), and let \( y \) be local maximum of \( v - \phi \). There exists a neighborhood of \( y \), say \( N_y \), such that
\[
v(x) - \phi(x) \leq v(y) - \phi(y), \quad x \in N_y.
\]
Thus,
\[
v(x) - v(y) \leq \phi(x) - \phi(y), \quad x \in N_y.
\]
 \pause By Lemma 3, for any control policy \( u_t \in U \) there exists \( t_y \) small enough such that  
\[
X_0 = y, \quad X_t \in N_y, \quad \forall 0 \leq t \leq t_y.
\]  
Thus,  
\[
v(X^y_t) - v(y) \leq \phi(X^y_t) - \phi(y), \quad \forall 0 \leq t \leq t_y.
\]  
    }
   
    
\end{frame}
\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
    
For any small \(\varepsilon > 0\), the DPP ensures existence of an \(\varepsilon\)-optimal control policy \( u_s^{\varepsilon,t} \) such that
\[
v(y) \leq \int_0^t e^{-\beta s} c(X_s^y, u_s^{\varepsilon,t}) \, ds + e^{-\beta t} v(X_t^y) + \varepsilon t.
\]
\par This is the "reverse" inequality of the supersolution case, since \( v(y) \) is a supremum and this control nearly attains that supremum. 
\vspace{1em}
\par  \pause Therefore, adding the two inequalities leads to, \( 0 \leq t \leq t_y, \)  
\[
v(X_t^y) \leq \int_0^t e^{-\beta s} c(X_s^y, u_s^{\varepsilon,t}) \, ds + e^{-\beta t} v(X_t^y) + \phi(X_t^y) - \phi(y) + \varepsilon t.
\] 
Rearranging:
\[
v(X_t^y)(1 - e^{-\beta t}) - \int_0^t e^{-\beta s} c(X_s^y, u_s^{\varepsilon,t}) \, ds \leq \phi(X_t^y) - \phi(y) + \varepsilon t. \tag{14.8}
\]
 \pause The complication here is that the control policy \( u_s^{\varepsilon,t} \) depends on \( t \). Thus, we have to be careful when taking the limit \( t \to 0 \) in (14.8). For \( 0 \leq t \leq t_y \), we have by the Lipschitz condition of \( f \):
    }
   
    
\end{frame}

\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
    \[
\|f(X_s^y, u_s^{\varepsilon,t}) - f(y, u_s^{\varepsilon,t})\| \leq L \|X_s^y - y\|,
\]
and
\[
\phi(X_t^y) - \phi(y) = \int_0^t D\phi(X_s^y) \cdot f(X_s^y, u_s^{\varepsilon,t}) \, ds = \int_0^t D\phi(y) \cdot f(y, u_s^{\varepsilon,t}) \, ds + o(t), \tag{14.9}
\]
 \pause because
    {\footnotesize \scriptsize
    \begin{align*}
        &\frac{1}{t} \left| \int_{0}^{t} \{ D\phi(X_s^y) 
        \cdot f(X_s^y, u_s^{\varepsilon,t}) - D\phi(y) \cdot f(y, u_s^{\varepsilon,t}) \} \, ds \right|\\
        &= \frac{1}{t} \left| \int_{0}^{t} \{ D\phi(X_s^y) \cdot (f(X_s^y, u_s^{\varepsilon,t}) - f(y, u_s^{\varepsilon,t})) + (D\phi(X_s^y) 
        - D\phi(y)) \cdot f(y, u_s^{\varepsilon,t}) \} \, ds \right|\\
        & \leq \frac{1}{t} \int_{0}^{t} |D\phi(X_s^y) \cdot (f(X_s^y, u_s^{\varepsilon,t}) 
        - f(y, u_s^{\varepsilon,t}))| \, ds 
        + \frac{1}{t} \int_{0}^{t} |(D\phi(X_s^y) 
        - D\phi(y)) \cdot f(y, u_s^{\varepsilon,t})| \, ds\\
        &\leq \frac{1}{t} \int_{0}^{t} \|D\phi(X_s^y)\| \cdot \|f(X_s^y, u_s^{\varepsilon,t}) - f(y, u_s^{\varepsilon,t})\| \, ds 
        + \frac{1}{t} \int_{0}^{t} |(D\phi(X_s^y) - D\phi(y)) \cdot f(y, u_s^{\varepsilon,t})| \, ds\\
        &\leq \frac{L}{t} \int_{0}^{t} \|D\phi(X_s^y)\| \cdot \|X_s^y - y\| \, ds 
        + M_y \frac{1}{t} \int_{0}^{t} \|D\phi(X_s^y) - D\phi(y)\| \, ds
    \end{align*}

    }

    }
   
    
\end{frame}

\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
     as \( t \to 0 \), where \( M_y = \sup_{u \in U} \|f(y, u)\| \), because \( D\phi(X^y_s) \to D\phi(y) \) and \( X^y_s \to y \) as \( s \to 0 \). 

Thus, (14.8) and (14.9) yield
\[
v(X^y_t)(1 - e^{-\beta t}) - \int_0^t e^{-\beta s} c(X^y_s, u_s^{\varepsilon,t}) \, ds - \int_0^t D\phi(y) \cdot f(y, u_s^{\varepsilon,t}) \, ds \leq o(t) + t\varepsilon. \tag{14.10}
\]
 \pause Similarly, the Lipschitz condition of \( c \) gives
\[
\int_0^t e^{-\beta s} c(X^y_s, u_s^{\varepsilon,t}) \, ds = \int_0^t c(y, u_s^{\varepsilon,t}) \, ds + o(t). \tag{14.11}
\]
and the uniform continuity of \( v \) leads to
\[
v(X^y_t)(1 - e^{-\beta t}) = \beta t v(y) + o(t) = \int_0^t \beta v(y) \, ds + o(t). \tag{14.12}
\]
 \pause Combining (14.10)-(14.12) yields
\[
\int_0^t \left[ \beta v(y) - D\phi(y) \cdot f(y, u_s^{\varepsilon,t}) - c(y, u_s^{\varepsilon,t}) \right] ds \leq t\varepsilon + o(t).
\]
    }
   
    
\end{frame}

\begin{frame}{The Verification Result}


    {\footnotesize \footnotesize
     Thus, the inf of the integrand inside must satisfy
\[
\inf_{u \in U} \left[ \beta v(y) - D\phi(y) \cdot f(y, u) - c(y, u) \right] \leq \varepsilon.
\]

In other words,
\[
\beta v(y) - \sup_{u \in U} \left\{ D\phi(y) \cdot f(y, u) + c(y, u) \right\} \leq \varepsilon.
\]

Since \( \varepsilon > 0 \) is arbitrary, we have that \( v \) is 
a subsolution of the HJB equation. 

    }
   
    
\end{frame}


\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
      Theorem 2: (The Comparison Principle). Let \( u_1 \) be any viscosity subsolution 
      to the HJB equation and \( u_2 \) be any viscosity supersolution to the HJB equation. 
      If \( u_1 \) and \( u_2 \) are both uniformly bounded, then \( u_1 \leq u_2 \).
    \[
    \beta v(x) - \sup_{u \in U} [Dv(x) \cdot f(x,u) + c(x, u)] = 0.
    \]
\par  \pause Theorem 2 is the most difficult part of the proof of Theorem 1. 
The proof of Theorem 2 relies on a doubling variable technique from Kruzkov (1970), 
which introduces a new function \( \Psi \) of two variables in \( u_1(x) \) and \( u_2(y) \). 
The function \( \Psi \) allows us to treat \( u_2 \) (resp. \( u_1 \)) 
as a constant with respect to the test function of \( u_1 \) (resp. \( u_2 \)).
\vspace{1em}
\par \pause  We shall prove Theorem 2 by contradiction. Suppose there exists \( \delta > 0 \) 
and \( z \in \mathbb{R}^d \) such that \( u_1(z) - u_2(z) \geq 2\delta \).
We will try to derive a contradiction from this assumption using the “doubling variable” trick.
\vspace{1em}
\par  \pause Consider a (doubling variable) function \(\Psi : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}\) defined as
\[
\Psi(x, y; \varepsilon, \alpha, p) = -u_1(x) + u_2(y) + \frac{\|x - y\|^2}{2\varepsilon} + \alpha \left[ \left( 1 + \|x\|^2 \right)^p + \left( 1 + \|y\|^2 \right)^p \right],
\]
    }
   
    
\end{frame}
\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
    \par  where the constants \(\varepsilon, \alpha, p > 0\). The key idea of the proof is to show a contradiction by using the fact that, when \(\varepsilon \to 0\) and \(\alpha \to 0\), the minimum of \(\Psi(x, y; \varepsilon, \alpha, p)\) should be achieved when \(x = y\) with the value \(u_2(x) - u_1(x)\). The term \(\left( 1 + \|x\|^2 \right)^p + \left( 1 + \|y\|^2 \right)^p\) is added to ensure that \(\Psi\) has a minimum point.
    \vspace{1em}
    \par \pause  Choose \(\alpha\) small enough so that \(2\alpha \left( 1 + \|z\|^2 \right)^p \leq \delta\)
\[
\Psi(z, z; \varepsilon, \alpha, p) = -u_1(z) + u_2(z) + 2\alpha \left( 1 + \|z\|^2 \right)^p \leq -2\delta + \delta = -\delta.
\]
We choose \(p > 0\) so small that
\[
p \left( 2L + M \right) \leq \beta,\tag{14.13}
\]
\par where \(M = \sup_{u \in U} \|f(0, u)\|\). Note that \(\alpha\) and \(p\) do not depend on \(\varepsilon\). 
\vspace{1em}
\par  \pause The definition of \(\Psi\) implies that
\[
\lim_{\|x \|\to \infty} \Psi(x, y; \varepsilon, \alpha, p) \to \infty, \quad \lim_{\|y \|\to \infty} \Psi(x, y; \varepsilon, \alpha, p) \to \infty,
\]
 }
   
    
\end{frame}
\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
    \par Because \(u_1\) and \(u_2\) are bounded by the assumption in the theorem. Therefore, there must exist a pair \((\bar{x}, \bar{y}) \in K_\varepsilon\) that attain the minimum of the continuous function \(\Psi\), where \(K_\varepsilon\) is a compact set. In particular,
    \[
\Psi(\bar{x}, \bar{y}; \varepsilon, \alpha, p) = \inf_{x, y}
\Psi(x, y; \varepsilon, \alpha, p) \leq \Psi(z, z; \varepsilon, \alpha, p) \leq -\delta. \tag{14.14}
\]
Lemma 4: For any \(\varepsilon > 0\), we have
\[
\beta \Psi (\bar{x}, \bar{y}; \varepsilon, \alpha, p) \geq -\frac{L}{\varepsilon} \| \bar{x} - \bar{y} \|^2 - L \| \bar{x} - \bar{y} \| - \varepsilon. \tag{14.15}
\]
 \pause 
\par [Proof]
\par Introduce the notation
\[
H(x, p) = \sup_{u \in U} [f(x, u) \cdot p + c(x, u)].
\]
Then the HJB equation can be written as
\[
\beta v(x) - H(x, Dv(x)) = 0.
\]

 \pause (i) Consider a test function
\[
\phi (x) = u_2 (\bar{y}) + \frac{\| x - \bar{y} \|^2}{2\varepsilon} + \alpha \left[ \left( 1 + \| x \|^2 \right)^p + \left( 1 + \| \bar{y} \|^2 \right)^p \right].
\]

    }
   
    
\end{frame}

\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
    Note that  
\[
\Psi(x, \bar{y}; \varepsilon, \alpha, p) = -u_1(x) + \phi(x).
\]

     Thus, \(-u_1(x) + \phi(x)\) attains its minimum at \(\bar{x}\). Hence, \(u_1(x) - \phi(x)\) attains its maximum at \(\bar{x}\). By the definition of the subsolution,  
\[
\beta u_1(\bar{x}) - H(\bar{x}, D\phi(\bar{x})) \leq 0.
\]

 \pause However,  
\[
D\phi(x) = \frac{x - \bar{y}}{\varepsilon} + \alpha \left[ 2p \left( 1 + \|x\|^2 \right)^{p-1} x \right].
\]

Thus,  
\[
\beta u_1(\bar{x}) \leq H(\bar{x}, D\phi(\bar{x})) = H \left( \bar{x}, \frac{\bar{x} - \bar{y}}{\varepsilon} + \alpha \left[ 2p \left( 1 + \|\bar{x}\|^2 \right)^{p-1} \bar{x} \right] \right),
\]
from which we get, by the definition of sup in \(H(x, p)\)  
\[
\beta u_1(\bar{x}) \leq f(\bar{x}, u_\varepsilon) \cdot \left\{ \frac{\bar{x} - \bar{y}}{\varepsilon} + \alpha \left[ 2p \left( 1 + \|\bar{x}\|^2 \right)^{p-1} \bar{x} \right] \right\} + c(\bar{x}, u_\varepsilon) + \varepsilon, \quad \exists u_\varepsilon \in U. \tag{14.16}
\]
    }
   
    
\end{frame}

\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
     Similarly, consider a test function
\[
\tilde{\phi}(y) = u_1(\bar{x}) - \frac{\|\bar{x} - y\|^2}{2\varepsilon} - \alpha \left[ \left( 1 + \|\bar{x}\|^2 \right)^p + \left( 1 + \|y\|^2 \right)^p \right].
\]

Note that
\[
\Psi(\bar{x}, y; \varepsilon, \alpha, p) = u_2(y) - \tilde{\phi}(y).
\]

Thus, \(u_2(y) - \tilde{\phi}(y)\) attains its minimum at \(\bar{y}\). By the definition of the supersolution,
\[
\beta u_2(\bar{y}) - H(\bar{y}, D\tilde{\phi}(\bar{y})) \geq 0.
\]
 \pause However,
\[
D\tilde{\phi}(y) = \frac{\bar{x} - y}{\varepsilon} + \alpha \left[ 2p \left( 1 + \|y\|^2 \right)^{p-1} y \right].
\]

Thus,
\[
\beta u_2(\bar{y}) \geq H \left( \bar{y}, \frac{\bar{x} - \bar{y}}{\varepsilon} + \alpha \left[ 2p \left( 1 + \|\bar{y}\|^2 \right)^{p-1} \bar{y} \right] \right),
\]
from which we get
\[
\beta u_2(\bar{y}) \geq f(\bar{y}, u) \cdot \left\{ \frac{\bar{x} - \bar{y}}{\varepsilon} + \alpha \left[ 2p \left( 1 + \|\bar{y}\|^2 \right)^{p-1} \bar{y} \right] \right\} + c(\bar{y}, u), \quad \forall u \in U. \tag{14.17}
\]
    }
   
    
\end{frame}
\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
      Combine (14.16) and (14.17) gives
      \begin{align*}
        &\beta \left( u_1 (\bar{x}) - u_2 (\bar{y}) \right)\\
        \leq &f(\bar{x}, u_\varepsilon) \cdot \left\{ \frac{\bar{x} - \bar{y}}{\varepsilon} + \alpha \left[ 2p \left( 1 + \|\bar{x}\|^2 \right)^{p-1} \bar{x} \right] \right\} + c(\bar{x}, u_\varepsilon)\\
        &- f(\bar{y}, u_\varepsilon) \cdot \left\{ \frac{\bar{x} - \bar{y}}{\varepsilon} + \alpha \left[ 2p \left( 1 + \|\bar{y}\|^2 \right)^{p-1} \bar{y} \right] \right\} - c(\bar{y}, u_\varepsilon) + \varepsilon\\
        =& (f(\bar{x}, u_\varepsilon) - f(\bar{y}, u_\varepsilon)) \cdot \frac{\bar{x} - \bar{y}}{\varepsilon} + c(\bar{x}, u_\varepsilon) - c(\bar{y}, u_\varepsilon) + \varepsilon\\
        &+ \alpha 2p \left( 1 + \|\bar{x}\|^2 \right)^{p-1} f(\bar{x}, u_\varepsilon) \cdot \bar{x} - \alpha 2p \left( 1 + \|\bar{y}\|^2 \right)^{p-1} f(\bar{y}, u_\varepsilon) \cdot \bar{y}.\\
      \end{align*}
       \pause Note that for the first two terms of the above equation,
\[
\left| (f(\bar{x}, u_\varepsilon) - f(\bar{y}, u_\varepsilon)) \cdot \frac{\bar{x} -
 \bar{y}}{\varepsilon} \right| \leq \frac{1}{\varepsilon} \|f(\bar{x}, u_\varepsilon) 
 - f(\bar{y}, u_\varepsilon)\| \cdot \|\bar{x} - \bar{y}\|\leq \frac{L}{\varepsilon} \|\bar{x} - \bar{y}\|^2,
\]

\[
|c(\bar{x}, u) - c(\bar{y}, u)| \leq L \|\bar{x} - \bar{y}\|.
\]

 \pause In addition, we have by Lipschitz 
\[
\|f(x, u_{\varepsilon})\| \leq \|f(x, u_{\varepsilon}) - f(0, u_{\varepsilon})\| + M \leq L \|x\| + M.
\]


    }
   
    
\end{frame}

\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
     Thus, the third term has an upper bound
\[
|f(x, u) \cdot x| \leq \|f(x, u)\|\|x\| \leq (L\|x\| + M)\|x\| = L\|x\|^2 + M\|x\|.
\]
    \par Then 
    \[
    L\|x\|^2 + M\|x\| = \frac{L\|x\|^2 + M\|x\|}{1 + \|x\|^2}(1 + \|x\|^2) \leq \left(L + \frac{M}{2}\right)(1 + \|x\|^2),
    \]
      \pause  Hence
\[
\left( 1 + \|\bar{x}\|^2 \right)^{p-1} f(\bar{x}, u_{\varepsilon}) \cdot \bar{x} \leq \left( L + \frac{M}{2} \right) \left( 1 + \|\bar{x}\|^2 \right)^p,
\]
\[
\left( 1 + \|\bar{y}\|^2 \right)^{p-1} f(\bar{y}, u_{\varepsilon}) \cdot \bar{y} \leq \left( L + \frac{M}{2} \right) \left( 1 + \|\bar{y}\|^2 \right)^p.
\]
 \pause Putting all pieces together we have
\begin{align*}
    &\beta (u_1 (\bar{x}) - u_2 (\bar{y}))\\
    &\leq \frac{L}{\varepsilon} \| \bar{x} - \bar{y} \|^2 + L \| \bar{x} - \bar{y} \| + \alpha p (2L + M) \left\{ \left( 1 + \|\bar{x}\|^2 \right)^p + \left( 1 + \|\bar{y}\|^2 \right)^p \right\} + \varepsilon\\
    &\leq \frac{L}{\varepsilon} \| \bar{x} - \bar{y} \|^2 + L \| \bar{x} - \bar{y} \| + \alpha \beta \left\{ \left( 1 + \|\bar{x}\|^2 \right)^p + \left( 1 + \|\bar{y}\|^2 \right)^p \right\} + \varepsilon,
\end{align*}

because \( p (2L + M) \leq \beta \) via (14.13).
    }
 
    
\end{frame}
\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
      In summary, we have
      \begin{align*}
        &\beta \Psi (\bar{x}, \bar{y}; \varepsilon, \alpha, p)\\
        &= -\beta u_1(\bar{x}) + \beta u_2(\bar{y}) + \frac{\beta \|\bar{x} - \bar{y}\|^2}{2\varepsilon} + \alpha \beta \left[ \left( 1 + \|\bar{x}\|^2 \right)^p + \left( 1 + \|\bar{y}\|^2 \right)^p \right]\\
        &\geq -\frac{L}{\varepsilon} \|\bar{x} - \bar{y}\|^2 - L \|\bar{x} - \bar{y}\| + \frac{\beta \|\bar{x} - \bar{y}\|^2}{2\varepsilon} - \varepsilon\\
        &\geq -\frac{L}{\varepsilon} \|\bar{x} - \bar{y}\|^2 - L \|\bar{x} - \bar{y}\| - \varepsilon,
      \end{align*}
    \par from which the lemma is proved. 
    \vspace{1em} \pause 
    \par [Proof of Theorem 2]
    \par Recall the setup, we assumed by contradiction that there exists a point \( z \in \mathbb{R}^d \) and some \(\delta > 0\) such that  
\[
u_1(z) - u_2(z) \geq 2\delta.
\]

The goal is to show this cannot happen.
    }
 
    
\end{frame}
\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
   To use the doubling-variable technique, we defined the auxiliary function  
\[
\Psi(x, y; \varepsilon, \alpha, p) = -u_1(x) + u_2(y) + \frac{\|x - y\|^2}{2\varepsilon} + \alpha \left[ (1 + \|x\|^2)^p + (1 + \|y\|^2)^p \right],
\]  
and chose \((\bar{x}, \bar{y})\) such that \(\Psi\) attains its minimum:  
\[
\Psi(\bar{x}, \bar{y}; \varepsilon, \alpha, p) = \inf_{x,y} \Psi(x, y; \varepsilon, \alpha, p).
\]
 \pause (14.14) and (14.15) imply that
\[
-\beta \delta \geq -\frac{L}{\varepsilon} \|\bar{x} - \bar{y}\|^2 - L \|\bar{x} - \bar{y}\| - \varepsilon. \tag{14.18}
\]

We shall prove that
\[
\frac{\|\bar{x} - \bar{y}\|^2}{\varepsilon} \to 0, \quad \varepsilon \to 0. \tag{14.19}
\]
If this holds, then the right-hand side of (14.18) tends to 0, and since \(\delta > 0\) and \(\beta > 0\), we get \(-\beta \delta \geq 0\), an impossibility — hence \(u_1 \leq u_2\).
    }
 
    
\end{frame}

\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
    Since \((\bar{x}, \bar{y})\) is the argmin of \(\Psi (\bar{x}, \bar{y}; \varepsilon, \alpha, p)\), we have
\[
2 \Psi (\bar{x}, \bar{y}; \varepsilon, \alpha, p) \leq \Psi (\bar{x}, \bar{x}; \varepsilon, \alpha, p) + \Psi (\bar{y}, \bar{y}; \varepsilon, \alpha, p),
\]
Plug in the definition $\Psi $ on both side
\[
-2u_1(\bar{x}) + 2u_2(\bar{y}) + \frac{\|\bar{x} - \bar{y}\|^2}{\varepsilon} + 2\alpha \left[ \left( 1 + \|\bar{x}\|^2 \right)^p + \left( 1 + \|\bar{y}\|^2 \right)^p \right]
\]
\[
\leq -u_1(\bar{x}) + u_2(\bar{x}) - u_1(\bar{y}) + u_2(\bar{y}) + 2\alpha \left[ \left( 1 + \|\bar{x}\|^2 \right)^p + \left( 1 + \|\bar{y}\|^2 \right)^p \right],
\]
 \pause Cancel the identical $\alpha$-terms and rearrange
\[
\frac{\|\bar{x} - \bar{y}\|^2}{\varepsilon} \leq u_1(\bar{x}) - u_1(\bar{y}) + u_2(\bar{x}) - u_2(\bar{y}). \tag{14.20}
\]
First the boundedness of \( u_1 \) and \( u_2 \) implies that
\[
\|\bar{x} - \bar{y}\|^2 \to 0, \quad \varepsilon \to 0,
\]
    }
 
    
\end{frame}

\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
    because the right side of (14.20) is bounded. Next, since \( (\bar{x}, \bar{y}) \) is in a compact set \( K_\varepsilon \) and \( \|\bar{x} - \bar{y}\|^2 \to 0 \) (as is just proved), the continuity of \( u_1 \) and \( u_2 \) in the definition of subsolution and supersolution implies that
\[
u_1(\bar{x}) - u_1(\bar{y}) + u_2(\bar{x}) - u_2(\bar{y}) \to 0,
\]
as \( \varepsilon \to 0 \). Thus, we get (14.19) via (14.20). In summary, letting \( \varepsilon \to 0 \) we get \( \beta \delta \leq 0 \) from (14.18), a contradiction.
\vspace{1em}
 \pause 
\par [Proof of Theorem 1]
\par Theorem 1 says that the value function \( v \) of the control problem  
\[
\beta v(x) - \sup_{u \in U} \{ Dv(x) \cdot f(x, u) + c(x, u) \} = 0 \tag{14.5}
\]  
is the unique bounded viscosity solution of the HJB equation.  

We already showed in previous propositions that \( v \) is
a viscosity supersolution, and  a viscosity subsolution. So existence is done.  

Now we need uniqueness.  
    }
 
    
\end{frame}

\begin{frame}{Uniqueness via the Comparison Principle}


    {\footnotesize \footnotesize
    Assume for contradiction that there's another bounded viscosity solution \( v_1 \).  
Both \( v \) and \( v_1 \) satisfy (14.5). We will use the Comparison Principle (Theorem 2):  
\vspace{1em}
\par If \( v_1 \) is a subsolution and \( v \) is a supersolution, and both are bounded, then \( v_1 \leq v \).  by Theorem 2 we get  
\[
v_1 \leq v.
\]  
 \pause But \( v_1 \) also satisfies the same PDE,  
so it is both a subsolution and a supersolution.  
Hence \( v \) can be seen as the subsolution and \( v_1 \) the supersolution, giving  
\[
v \leq v_1.
\]  

Thus,  
\[
v = v_1.
\]  

\par That is, the value function \( v \) is unique among all bounded viscosity solutions.
\vspace{1em}
\par  \pause Remark: Instead of requiring continuity in the definition of the sub-
solution and supersolution, the subsolution can be relaxed to be upper semi-
continuous and the supersoluton to be lower semi-continuous. Theorem 1 still
holds with this relaxation.
    }
 
    
\end{frame}
% \begin{frame}

%     {\footnotesize \footnotesize

%     }
    
% \end{frame}
% % {\mathbb{P}^*}
% \tilde{\mathbb{P}}
% {\footnotesize \footnotesize
% }
% \tiny
% \scriptsize
% \footnotesize
% \small
% \normalsize (default)
\end{document}